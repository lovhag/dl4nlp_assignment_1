{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURE\n",
    "1. Load the training data.\n",
    "2. Adapt data to BERT\n",
    "3. Obtain a pretrained BERT and fine-tune.\n",
    "4. Evaluate BERT results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the directory you wish to save ALL the data to: BERT_results_final\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'BERT_results_final'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9a603dbe4e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDIRECTORY_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specify the directory you wish to save ALL the data to: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIRECTORY_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'BERT_results_final'"
     ]
    }
   ],
   "source": [
    "DIRECTORY_NAME = input(\"Specify the directory you wish to save ALL the data to: \")\n",
    "os.makedirs(DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1 = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_X_data_1_train.pickle\")\n",
    "X_data_2 = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_X_data_2_train.pickle\")\n",
    "y_data = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_y_data_train.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1_test = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_X_data_1_test.pickle\")\n",
    "X_data_2_test = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_X_data_2_test.pickle\")\n",
    "y_data_test = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/with_eval_y_data_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples: 121678\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data samples: {len(y_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( vii ) BGL - General Trust Fund for the Core Programme Budget for the Biosafety Protocol , which is extended through 31 December 2011 ; ( viii ) BHL - Special Voluntary Trust Fund for Additional Voluntary Contributions in Support of Approved Activities of the Biosafety Protocol , which is extended through 31 December 2011 ; ( ix ) BTL - General Trust Fund for the Conservation of European Bats ( EUROBATS ) , which is extended through 31 December 2014 ;'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following consultation in 2007 , the Government is considering replacing the current legislation with a single Equality Act . As part of this , it is considering the case for extending protection from age discrimination outside the workplace and for extending positive duties on public authorities to the other protected grounds . In Northern Ireland , additional protections have been established to promote equality .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adapt data to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(X_data_1, X_data_2, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(X_data_1_test, X_data_2_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SenseDataset(train_encodings, y_data)\n",
    "test_dataset = SenseDataset(test_encodings, y_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121678"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2112,  1045,  1011,  4955, 24497,  2177,  2003,  1037,  2877,\n",
       "          1010,  2248,  1010,  2512,  1011, 10605,  5502,  4056,  2000, 12771,\n",
       "          2152,  3737,  1010,  2711,  1011, 16441,  2740,  1998,  2591,  2729,\n",
       "          1010,  2731,  1010,  2495,  1010,  6107,  1010, 11252,  1998,  3293,\n",
       "          2578,  2000,  2111,  2007, 13597,  1010,  3080,  2111,  1998,  2500,\n",
       "          2040,  2024, 14785,  3550,  1012,  2144,  2494,  1010, 24497,  2177,\n",
       "          2038,  5281,  6196,  4935,  1012,   102,  2011,  2555,  1010,  5120,\n",
       "          2001,  2034,  1999,  2885,  1010,  2007,  1996,  5569,  3072,  4577,\n",
       "          3284,  2426,  1996,  2647,  3032,  2007, 18730,  1999,  6653,  1012,\n",
       "          1999,  3088,  1010,  5279,  2001,  2877,  1010,  2007,  2148,  3088,\n",
       "          4577,  3284,  2426,  1996,  3032,  1997,  4942,  1011, 24505,  3088,\n",
       "          1012,  1999,  4021,  1010,  5264,  2001,  2877,  1010,  2004,  2001,\n",
       "          4380,  1999,  2148,  2637,  1010,  2007,  1996, 17094,  3805,  1997,\n",
       "          1996,  2060,  7139,  3032,  1998,  6849, 11509,  3805,  1997,  1996,\n",
       "          2717,  1997,  2430,  2637,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_with_pickle(data_dict, folder_name=None):\n",
    "    if not folder_name:\n",
    "        folder_name = input(f\"Specify which prefix filename you wish to save {list(data_dict.keys())} to: \")\n",
    "    if folder_name:\n",
    "        for key, value in data_dict.items():\n",
    "            filename = folder_name+\"/\"+key+\".pickle\"\n",
    "            with open(filename, \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(value, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which folder should the datasets be saved to?: /home/lovhag/storage/data/BERT_data_final/\n"
     ]
    }
   ],
   "source": [
    "save_datasets_to_folder = input(\"Which folder should the datasets be saved to?: \")\n",
    "save_data_with_pickle({\"train_dataset\": train_dataset, \"test_dataset\": test_dataset}, save_datasets_to_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain a pretrained BERT and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)\n",
    "\n",
    "train_dataset = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/train_dataset.pickle\")\n",
    "test_dataset = load_pickle_data(\"/home/lovhag/storage/data/BERT_data_final/test_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3_new/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/opt/miniconda3_new/lib/python3.8/importlib/__init__.py:127: H5pyDeprecationWarning: The h5py.highlevel module is deprecated, code should import directly from h5py, e.g. 'from h5py import File'.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ").to(device)\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "#for param in model.base_model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "# keep the weights of the embedding layer frozen\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction\n",
    "\n",
    "model = BertForNextSentencePrediction.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.  \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ").to(device)\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# keep the weights of the embedding layer frozen\n",
    "#for param in model.bert.embeddings.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_params(model):\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "cls.seq_relationship.weight                                 (2, 768)\n",
      "cls.seq_relationship.bias                                       (2,)\n"
     ]
    }
   ],
   "source": [
    "print_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "def get_train_dataloader(train_dataset, batch_size):\n",
    "    return DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "def get_validation_dataloader(validation_dataset, batch_size):\n",
    "    return DataLoader(\n",
    "                validation_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "def train(model, epochs, batch_size, optimizer, train_dataset, validation_dataset, use_labels=True):\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    train_dataloader = get_train_dataloader(train_dataset, batch_size)\n",
    "    validation_dataloader = get_validation_dataloader(validation_dataset, batch_size)\n",
    "    \n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            #print(f\"step: {step}\")\n",
    "            \n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[\"input_ids\"].to(device)\n",
    "            b_token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            b_input_mask = batch[\"attention_mask\"].to(device)\n",
    "            b_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            if use_labels:\n",
    "                loss, logits = model(b_input_ids, \n",
    "                                     token_type_ids=b_token_type_ids, \n",
    "                                     attention_mask=b_input_mask, \n",
    "                                     labels=b_labels)\n",
    "            else:\n",
    "                loss, logits = model(b_input_ids, \n",
    "                                     token_type_ids=b_token_type_ids, \n",
    "                                     attention_mask=b_input_mask, \n",
    "                                     next_sentence_label=b_labels)\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[\"input_ids\"].to(device)\n",
    "            b_token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            b_input_mask = batch[\"attention_mask\"].to(device)\n",
    "            b_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                if use_labels:\n",
    "                    loss, logits = model(b_input_ids, \n",
    "                                         token_type_ids=b_token_type_ids, \n",
    "                                         attention_mask=b_input_mask, \n",
    "                                         labels=b_labels)\n",
    "                else:\n",
    "                    loss, logits = model(b_input_ids, \n",
    "                                         token_type_ids=b_token_type_ids, \n",
    "                                         attention_mask=b_input_mask, \n",
    "                                         next_sentence_label=b_labels)\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,605.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  7,605.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  7,605.    Elapsed: 0:00:57.\n",
      "  Batch   160  of  7,605.    Elapsed: 0:01:16.\n",
      "  Batch   200  of  7,605.    Elapsed: 0:01:36.\n",
      "  Batch   240  of  7,605.    Elapsed: 0:01:57.\n",
      "  Batch   280  of  7,605.    Elapsed: 0:02:17.\n",
      "  Batch   320  of  7,605.    Elapsed: 0:02:37.\n",
      "  Batch   360  of  7,605.    Elapsed: 0:02:57.\n",
      "  Batch   400  of  7,605.    Elapsed: 0:03:17.\n",
      "  Batch   440  of  7,605.    Elapsed: 0:03:37.\n",
      "  Batch   480  of  7,605.    Elapsed: 0:03:57.\n",
      "  Batch   520  of  7,605.    Elapsed: 0:04:17.\n",
      "  Batch   560  of  7,605.    Elapsed: 0:04:37.\n",
      "  Batch   600  of  7,605.    Elapsed: 0:04:57.\n",
      "  Batch   640  of  7,605.    Elapsed: 0:05:17.\n",
      "  Batch   680  of  7,605.    Elapsed: 0:05:37.\n",
      "  Batch   720  of  7,605.    Elapsed: 0:05:57.\n",
      "  Batch   760  of  7,605.    Elapsed: 0:06:17.\n",
      "  Batch   800  of  7,605.    Elapsed: 0:06:37.\n",
      "  Batch   840  of  7,605.    Elapsed: 0:06:57.\n",
      "  Batch   880  of  7,605.    Elapsed: 0:07:17.\n",
      "  Batch   920  of  7,605.    Elapsed: 0:07:37.\n",
      "  Batch   960  of  7,605.    Elapsed: 0:07:57.\n",
      "  Batch 1,000  of  7,605.    Elapsed: 0:08:17.\n",
      "  Batch 1,040  of  7,605.    Elapsed: 0:08:37.\n",
      "  Batch 1,080  of  7,605.    Elapsed: 0:08:57.\n",
      "  Batch 1,120  of  7,605.    Elapsed: 0:09:17.\n",
      "  Batch 1,160  of  7,605.    Elapsed: 0:09:37.\n",
      "  Batch 1,200  of  7,605.    Elapsed: 0:09:57.\n",
      "  Batch 1,240  of  7,605.    Elapsed: 0:10:17.\n",
      "  Batch 1,280  of  7,605.    Elapsed: 0:10:37.\n",
      "  Batch 1,320  of  7,605.    Elapsed: 0:10:57.\n",
      "  Batch 1,360  of  7,605.    Elapsed: 0:11:17.\n",
      "  Batch 1,400  of  7,605.    Elapsed: 0:11:37.\n",
      "  Batch 1,440  of  7,605.    Elapsed: 0:11:57.\n",
      "  Batch 1,480  of  7,605.    Elapsed: 0:12:17.\n",
      "  Batch 1,520  of  7,605.    Elapsed: 0:12:37.\n",
      "  Batch 1,560  of  7,605.    Elapsed: 0:12:57.\n",
      "  Batch 1,600  of  7,605.    Elapsed: 0:13:17.\n",
      "  Batch 1,640  of  7,605.    Elapsed: 0:13:38.\n",
      "  Batch 1,680  of  7,605.    Elapsed: 0:13:58.\n",
      "  Batch 1,720  of  7,605.    Elapsed: 0:14:18.\n",
      "  Batch 1,760  of  7,605.    Elapsed: 0:14:38.\n",
      "  Batch 1,800  of  7,605.    Elapsed: 0:14:58.\n",
      "  Batch 1,840  of  7,605.    Elapsed: 0:15:18.\n",
      "  Batch 1,880  of  7,605.    Elapsed: 0:15:38.\n",
      "  Batch 1,920  of  7,605.    Elapsed: 0:15:58.\n",
      "  Batch 1,960  of  7,605.    Elapsed: 0:16:18.\n",
      "  Batch 2,000  of  7,605.    Elapsed: 0:16:38.\n",
      "  Batch 2,040  of  7,605.    Elapsed: 0:16:58.\n",
      "  Batch 2,080  of  7,605.    Elapsed: 0:17:18.\n",
      "  Batch 2,120  of  7,605.    Elapsed: 0:17:38.\n",
      "  Batch 2,160  of  7,605.    Elapsed: 0:17:58.\n",
      "  Batch 2,200  of  7,605.    Elapsed: 0:18:18.\n",
      "  Batch 2,240  of  7,605.    Elapsed: 0:18:38.\n",
      "  Batch 2,280  of  7,605.    Elapsed: 0:18:58.\n",
      "  Batch 2,320  of  7,605.    Elapsed: 0:19:18.\n",
      "  Batch 2,360  of  7,605.    Elapsed: 0:19:38.\n",
      "  Batch 2,400  of  7,605.    Elapsed: 0:19:58.\n",
      "  Batch 2,440  of  7,605.    Elapsed: 0:20:18.\n",
      "  Batch 2,480  of  7,605.    Elapsed: 0:20:38.\n",
      "  Batch 2,520  of  7,605.    Elapsed: 0:20:58.\n",
      "  Batch 2,560  of  7,605.    Elapsed: 0:21:18.\n",
      "  Batch 2,600  of  7,605.    Elapsed: 0:21:38.\n",
      "  Batch 2,640  of  7,605.    Elapsed: 0:21:58.\n",
      "  Batch 2,680  of  7,605.    Elapsed: 0:22:18.\n",
      "  Batch 2,720  of  7,605.    Elapsed: 0:22:38.\n",
      "  Batch 2,760  of  7,605.    Elapsed: 0:22:58.\n",
      "  Batch 2,800  of  7,605.    Elapsed: 0:23:18.\n",
      "  Batch 2,840  of  7,605.    Elapsed: 0:23:38.\n",
      "  Batch 2,880  of  7,605.    Elapsed: 0:23:58.\n",
      "  Batch 2,920  of  7,605.    Elapsed: 0:24:18.\n",
      "  Batch 2,960  of  7,605.    Elapsed: 0:24:38.\n",
      "  Batch 3,000  of  7,605.    Elapsed: 0:24:58.\n",
      "  Batch 3,040  of  7,605.    Elapsed: 0:25:18.\n",
      "  Batch 3,080  of  7,605.    Elapsed: 0:25:38.\n",
      "  Batch 3,120  of  7,605.    Elapsed: 0:25:58.\n",
      "  Batch 3,160  of  7,605.    Elapsed: 0:26:18.\n",
      "  Batch 3,200  of  7,605.    Elapsed: 0:26:38.\n",
      "  Batch 3,240  of  7,605.    Elapsed: 0:26:58.\n",
      "  Batch 3,280  of  7,605.    Elapsed: 0:27:18.\n",
      "  Batch 3,320  of  7,605.    Elapsed: 0:27:38.\n",
      "  Batch 3,360  of  7,605.    Elapsed: 0:27:58.\n",
      "  Batch 3,400  of  7,605.    Elapsed: 0:28:18.\n",
      "  Batch 3,440  of  7,605.    Elapsed: 0:28:38.\n",
      "  Batch 3,480  of  7,605.    Elapsed: 0:28:58.\n",
      "  Batch 3,520  of  7,605.    Elapsed: 0:29:18.\n",
      "  Batch 3,560  of  7,605.    Elapsed: 0:29:38.\n",
      "  Batch 3,600  of  7,605.    Elapsed: 0:29:58.\n",
      "  Batch 3,640  of  7,605.    Elapsed: 0:30:18.\n",
      "  Batch 3,680  of  7,605.    Elapsed: 0:30:38.\n",
      "  Batch 3,720  of  7,605.    Elapsed: 0:30:58.\n",
      "  Batch 3,760  of  7,605.    Elapsed: 0:31:18.\n",
      "  Batch 3,800  of  7,605.    Elapsed: 0:31:38.\n",
      "  Batch 3,840  of  7,605.    Elapsed: 0:31:58.\n",
      "  Batch 3,880  of  7,605.    Elapsed: 0:32:18.\n",
      "  Batch 3,920  of  7,605.    Elapsed: 0:32:38.\n",
      "  Batch 3,960  of  7,605.    Elapsed: 0:32:58.\n",
      "  Batch 4,000  of  7,605.    Elapsed: 0:33:18.\n",
      "  Batch 4,040  of  7,605.    Elapsed: 0:33:38.\n",
      "  Batch 4,080  of  7,605.    Elapsed: 0:33:58.\n",
      "  Batch 4,120  of  7,605.    Elapsed: 0:34:18.\n",
      "  Batch 4,160  of  7,605.    Elapsed: 0:34:38.\n",
      "  Batch 4,200  of  7,605.    Elapsed: 0:34:58.\n",
      "  Batch 4,240  of  7,605.    Elapsed: 0:35:18.\n",
      "  Batch 4,280  of  7,605.    Elapsed: 0:35:38.\n",
      "  Batch 4,320  of  7,605.    Elapsed: 0:35:58.\n",
      "  Batch 4,360  of  7,605.    Elapsed: 0:36:18.\n",
      "  Batch 4,400  of  7,605.    Elapsed: 0:36:38.\n",
      "  Batch 4,440  of  7,605.    Elapsed: 0:36:58.\n",
      "  Batch 4,480  of  7,605.    Elapsed: 0:37:18.\n",
      "  Batch 4,520  of  7,605.    Elapsed: 0:37:38.\n",
      "  Batch 4,560  of  7,605.    Elapsed: 0:37:58.\n",
      "  Batch 4,600  of  7,605.    Elapsed: 0:38:18.\n",
      "  Batch 4,640  of  7,605.    Elapsed: 0:38:38.\n",
      "  Batch 4,680  of  7,605.    Elapsed: 0:38:58.\n",
      "  Batch 4,720  of  7,605.    Elapsed: 0:39:18.\n",
      "  Batch 4,760  of  7,605.    Elapsed: 0:39:38.\n",
      "  Batch 4,800  of  7,605.    Elapsed: 0:39:59.\n",
      "  Batch 4,840  of  7,605.    Elapsed: 0:40:19.\n",
      "  Batch 4,880  of  7,605.    Elapsed: 0:40:39.\n",
      "  Batch 4,920  of  7,605.    Elapsed: 0:40:59.\n",
      "  Batch 4,960  of  7,605.    Elapsed: 0:41:19.\n",
      "  Batch 5,000  of  7,605.    Elapsed: 0:41:39.\n",
      "  Batch 5,040  of  7,605.    Elapsed: 0:41:59.\n",
      "  Batch 5,080  of  7,605.    Elapsed: 0:42:19.\n",
      "  Batch 5,120  of  7,605.    Elapsed: 0:42:39.\n",
      "  Batch 5,160  of  7,605.    Elapsed: 0:42:59.\n",
      "  Batch 5,200  of  7,605.    Elapsed: 0:43:19.\n",
      "  Batch 5,240  of  7,605.    Elapsed: 0:43:39.\n",
      "  Batch 5,280  of  7,605.    Elapsed: 0:43:59.\n",
      "  Batch 5,320  of  7,605.    Elapsed: 0:44:19.\n",
      "  Batch 5,360  of  7,605.    Elapsed: 0:44:39.\n",
      "  Batch 5,400  of  7,605.    Elapsed: 0:44:59.\n",
      "  Batch 5,440  of  7,605.    Elapsed: 0:45:19.\n",
      "  Batch 5,480  of  7,605.    Elapsed: 0:45:39.\n",
      "  Batch 5,520  of  7,605.    Elapsed: 0:45:59.\n",
      "  Batch 5,560  of  7,605.    Elapsed: 0:46:19.\n",
      "  Batch 5,600  of  7,605.    Elapsed: 0:46:39.\n",
      "  Batch 5,640  of  7,605.    Elapsed: 0:46:59.\n",
      "  Batch 5,680  of  7,605.    Elapsed: 0:47:19.\n",
      "  Batch 5,720  of  7,605.    Elapsed: 0:47:39.\n",
      "  Batch 5,760  of  7,605.    Elapsed: 0:47:59.\n",
      "  Batch 5,800  of  7,605.    Elapsed: 0:48:19.\n",
      "  Batch 5,840  of  7,605.    Elapsed: 0:48:39.\n",
      "  Batch 5,880  of  7,605.    Elapsed: 0:48:59.\n",
      "  Batch 5,920  of  7,605.    Elapsed: 0:49:19.\n",
      "  Batch 5,960  of  7,605.    Elapsed: 0:49:39.\n",
      "  Batch 6,000  of  7,605.    Elapsed: 0:49:59.\n",
      "  Batch 6,040  of  7,605.    Elapsed: 0:50:19.\n",
      "  Batch 6,080  of  7,605.    Elapsed: 0:50:39.\n",
      "  Batch 6,120  of  7,605.    Elapsed: 0:50:59.\n",
      "  Batch 6,160  of  7,605.    Elapsed: 0:51:19.\n",
      "  Batch 6,200  of  7,605.    Elapsed: 0:51:39.\n",
      "  Batch 6,240  of  7,605.    Elapsed: 0:51:59.\n",
      "  Batch 6,280  of  7,605.    Elapsed: 0:52:19.\n",
      "  Batch 6,320  of  7,605.    Elapsed: 0:52:39.\n",
      "  Batch 6,360  of  7,605.    Elapsed: 0:52:59.\n",
      "  Batch 6,400  of  7,605.    Elapsed: 0:53:19.\n",
      "  Batch 6,440  of  7,605.    Elapsed: 0:53:39.\n",
      "  Batch 6,480  of  7,605.    Elapsed: 0:53:59.\n",
      "  Batch 6,520  of  7,605.    Elapsed: 0:54:19.\n",
      "  Batch 6,560  of  7,605.    Elapsed: 0:54:39.\n",
      "  Batch 6,600  of  7,605.    Elapsed: 0:54:59.\n",
      "  Batch 6,640  of  7,605.    Elapsed: 0:55:19.\n",
      "  Batch 6,680  of  7,605.    Elapsed: 0:55:39.\n",
      "  Batch 6,720  of  7,605.    Elapsed: 0:55:59.\n",
      "  Batch 6,760  of  7,605.    Elapsed: 0:56:19.\n",
      "  Batch 6,800  of  7,605.    Elapsed: 0:56:39.\n",
      "  Batch 6,840  of  7,605.    Elapsed: 0:56:59.\n",
      "  Batch 6,880  of  7,605.    Elapsed: 0:57:19.\n",
      "  Batch 6,920  of  7,605.    Elapsed: 0:57:39.\n",
      "  Batch 6,960  of  7,605.    Elapsed: 0:57:59.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7,000  of  7,605.    Elapsed: 0:58:19.\n",
      "  Batch 7,040  of  7,605.    Elapsed: 0:58:39.\n",
      "  Batch 7,080  of  7,605.    Elapsed: 0:58:59.\n",
      "  Batch 7,120  of  7,605.    Elapsed: 0:59:19.\n",
      "  Batch 7,160  of  7,605.    Elapsed: 0:59:39.\n",
      "  Batch 7,200  of  7,605.    Elapsed: 0:59:59.\n",
      "  Batch 7,240  of  7,605.    Elapsed: 1:00:19.\n",
      "  Batch 7,280  of  7,605.    Elapsed: 1:00:39.\n",
      "  Batch 7,320  of  7,605.    Elapsed: 1:00:59.\n",
      "  Batch 7,360  of  7,605.    Elapsed: 1:01:19.\n",
      "  Batch 7,400  of  7,605.    Elapsed: 1:01:39.\n",
      "  Batch 7,440  of  7,605.    Elapsed: 1:01:59.\n",
      "  Batch 7,480  of  7,605.    Elapsed: 1:02:19.\n",
      "  Batch 7,520  of  7,605.    Elapsed: 1:02:40.\n",
      "  Batch 7,560  of  7,605.    Elapsed: 1:03:00.\n",
      "  Batch 7,600  of  7,605.    Elapsed: 1:03:20.\n",
      "\n",
      "  Average training loss: 0.81\n",
      "  Training epcoh took: 1:03:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.59\n",
      "  Validation Loss: 0.66\n",
      "  Validation took: 0:14:52\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,605.    Elapsed: 0:00:20.\n",
      "  Batch    80  of  7,605.    Elapsed: 0:00:40.\n",
      "  Batch   120  of  7,605.    Elapsed: 0:01:00.\n",
      "  Batch   160  of  7,605.    Elapsed: 0:01:20.\n",
      "  Batch   200  of  7,605.    Elapsed: 0:01:40.\n",
      "  Batch   240  of  7,605.    Elapsed: 0:02:00.\n",
      "  Batch   280  of  7,605.    Elapsed: 0:02:20.\n",
      "  Batch   320  of  7,605.    Elapsed: 0:02:40.\n",
      "  Batch   360  of  7,605.    Elapsed: 0:03:00.\n",
      "  Batch   400  of  7,605.    Elapsed: 0:03:20.\n",
      "  Batch   440  of  7,605.    Elapsed: 0:03:40.\n",
      "  Batch   480  of  7,605.    Elapsed: 0:04:00.\n",
      "  Batch   520  of  7,605.    Elapsed: 0:04:20.\n",
      "  Batch   560  of  7,605.    Elapsed: 0:04:40.\n",
      "  Batch   600  of  7,605.    Elapsed: 0:05:00.\n",
      "  Batch   640  of  7,605.    Elapsed: 0:05:20.\n",
      "  Batch   680  of  7,605.    Elapsed: 0:05:40.\n",
      "  Batch   720  of  7,605.    Elapsed: 0:06:00.\n",
      "  Batch   760  of  7,605.    Elapsed: 0:06:20.\n",
      "  Batch   800  of  7,605.    Elapsed: 0:06:40.\n",
      "  Batch   840  of  7,605.    Elapsed: 0:07:00.\n",
      "  Batch   880  of  7,605.    Elapsed: 0:07:20.\n",
      "  Batch   920  of  7,605.    Elapsed: 0:07:40.\n",
      "  Batch   960  of  7,605.    Elapsed: 0:08:00.\n",
      "  Batch 1,000  of  7,605.    Elapsed: 0:08:20.\n",
      "  Batch 1,040  of  7,605.    Elapsed: 0:08:40.\n",
      "  Batch 1,080  of  7,605.    Elapsed: 0:09:00.\n",
      "  Batch 1,120  of  7,605.    Elapsed: 0:09:20.\n",
      "  Batch 1,160  of  7,605.    Elapsed: 0:09:40.\n",
      "  Batch 1,200  of  7,605.    Elapsed: 0:10:00.\n",
      "  Batch 1,240  of  7,605.    Elapsed: 0:10:21.\n",
      "  Batch 1,280  of  7,605.    Elapsed: 0:10:41.\n",
      "  Batch 1,320  of  7,605.    Elapsed: 0:11:01.\n",
      "  Batch 1,360  of  7,605.    Elapsed: 0:11:21.\n",
      "  Batch 1,400  of  7,605.    Elapsed: 0:11:41.\n",
      "  Batch 1,440  of  7,605.    Elapsed: 0:12:01.\n",
      "  Batch 1,480  of  7,605.    Elapsed: 0:12:21.\n",
      "  Batch 1,520  of  7,605.    Elapsed: 0:12:41.\n",
      "  Batch 1,560  of  7,605.    Elapsed: 0:13:01.\n",
      "  Batch 1,600  of  7,605.    Elapsed: 0:13:21.\n",
      "  Batch 1,640  of  7,605.    Elapsed: 0:13:41.\n",
      "  Batch 1,680  of  7,605.    Elapsed: 0:14:01.\n",
      "  Batch 1,720  of  7,605.    Elapsed: 0:14:21.\n",
      "  Batch 1,760  of  7,605.    Elapsed: 0:14:41.\n",
      "  Batch 1,800  of  7,605.    Elapsed: 0:15:01.\n",
      "  Batch 1,840  of  7,605.    Elapsed: 0:15:21.\n",
      "  Batch 1,880  of  7,605.    Elapsed: 0:15:41.\n",
      "  Batch 1,920  of  7,605.    Elapsed: 0:16:01.\n",
      "  Batch 1,960  of  7,605.    Elapsed: 0:16:21.\n",
      "  Batch 2,000  of  7,605.    Elapsed: 0:16:41.\n",
      "  Batch 2,040  of  7,605.    Elapsed: 0:17:01.\n",
      "  Batch 2,080  of  7,605.    Elapsed: 0:17:21.\n",
      "  Batch 2,120  of  7,605.    Elapsed: 0:17:41.\n",
      "  Batch 2,160  of  7,605.    Elapsed: 0:18:01.\n",
      "  Batch 2,200  of  7,605.    Elapsed: 0:18:21.\n",
      "  Batch 2,240  of  7,605.    Elapsed: 0:18:41.\n",
      "  Batch 2,280  of  7,605.    Elapsed: 0:19:01.\n",
      "  Batch 2,320  of  7,605.    Elapsed: 0:19:21.\n",
      "  Batch 2,360  of  7,605.    Elapsed: 0:19:41.\n",
      "  Batch 2,400  of  7,605.    Elapsed: 0:20:01.\n",
      "  Batch 2,440  of  7,605.    Elapsed: 0:20:21.\n",
      "  Batch 2,480  of  7,605.    Elapsed: 0:20:41.\n",
      "  Batch 2,520  of  7,605.    Elapsed: 0:21:01.\n",
      "  Batch 2,560  of  7,605.    Elapsed: 0:21:21.\n",
      "  Batch 2,600  of  7,605.    Elapsed: 0:21:41.\n",
      "  Batch 2,640  of  7,605.    Elapsed: 0:22:01.\n",
      "  Batch 2,680  of  7,605.    Elapsed: 0:22:21.\n",
      "  Batch 2,720  of  7,605.    Elapsed: 0:22:41.\n",
      "  Batch 2,760  of  7,605.    Elapsed: 0:23:01.\n",
      "  Batch 2,800  of  7,605.    Elapsed: 0:23:21.\n",
      "  Batch 2,840  of  7,605.    Elapsed: 0:23:41.\n",
      "  Batch 2,880  of  7,605.    Elapsed: 0:24:01.\n",
      "  Batch 2,920  of  7,605.    Elapsed: 0:24:21.\n",
      "  Batch 2,960  of  7,605.    Elapsed: 0:24:41.\n",
      "  Batch 3,000  of  7,605.    Elapsed: 0:25:01.\n",
      "  Batch 3,040  of  7,605.    Elapsed: 0:25:21.\n",
      "  Batch 3,080  of  7,605.    Elapsed: 0:25:41.\n",
      "  Batch 3,120  of  7,605.    Elapsed: 0:26:01.\n",
      "  Batch 3,160  of  7,605.    Elapsed: 0:26:21.\n",
      "  Batch 3,200  of  7,605.    Elapsed: 0:26:41.\n",
      "  Batch 3,240  of  7,605.    Elapsed: 0:27:01.\n",
      "  Batch 3,280  of  7,605.    Elapsed: 0:27:21.\n",
      "  Batch 3,320  of  7,605.    Elapsed: 0:27:41.\n",
      "  Batch 3,360  of  7,605.    Elapsed: 0:28:01.\n",
      "  Batch 3,400  of  7,605.    Elapsed: 0:28:21.\n",
      "  Batch 3,440  of  7,605.    Elapsed: 0:28:41.\n",
      "  Batch 3,480  of  7,605.    Elapsed: 0:29:01.\n",
      "  Batch 3,520  of  7,605.    Elapsed: 0:29:21.\n",
      "  Batch 3,560  of  7,605.    Elapsed: 0:29:41.\n",
      "  Batch 3,600  of  7,605.    Elapsed: 0:30:01.\n",
      "  Batch 3,640  of  7,605.    Elapsed: 0:30:21.\n",
      "  Batch 3,680  of  7,605.    Elapsed: 0:30:41.\n",
      "  Batch 3,720  of  7,605.    Elapsed: 0:31:01.\n",
      "  Batch 3,760  of  7,605.    Elapsed: 0:31:22.\n",
      "  Batch 3,800  of  7,605.    Elapsed: 0:31:42.\n",
      "  Batch 3,840  of  7,605.    Elapsed: 0:32:02.\n",
      "  Batch 3,880  of  7,605.    Elapsed: 0:32:22.\n",
      "  Batch 3,920  of  7,605.    Elapsed: 0:32:42.\n",
      "  Batch 3,960  of  7,605.    Elapsed: 0:33:02.\n",
      "  Batch 4,000  of  7,605.    Elapsed: 0:33:22.\n",
      "  Batch 4,040  of  7,605.    Elapsed: 0:33:42.\n",
      "  Batch 4,080  of  7,605.    Elapsed: 0:34:02.\n",
      "  Batch 4,120  of  7,605.    Elapsed: 0:34:22.\n",
      "  Batch 4,160  of  7,605.    Elapsed: 0:34:42.\n",
      "  Batch 4,200  of  7,605.    Elapsed: 0:35:02.\n",
      "  Batch 4,240  of  7,605.    Elapsed: 0:35:22.\n",
      "  Batch 4,280  of  7,605.    Elapsed: 0:35:42.\n",
      "  Batch 4,320  of  7,605.    Elapsed: 0:36:02.\n",
      "  Batch 4,360  of  7,605.    Elapsed: 0:36:22.\n",
      "  Batch 4,400  of  7,605.    Elapsed: 0:36:42.\n",
      "  Batch 4,440  of  7,605.    Elapsed: 0:37:02.\n",
      "  Batch 4,480  of  7,605.    Elapsed: 0:37:22.\n",
      "  Batch 4,520  of  7,605.    Elapsed: 0:37:42.\n",
      "  Batch 4,560  of  7,605.    Elapsed: 0:38:02.\n",
      "  Batch 4,600  of  7,605.    Elapsed: 0:38:22.\n",
      "  Batch 4,640  of  7,605.    Elapsed: 0:38:42.\n",
      "  Batch 4,680  of  7,605.    Elapsed: 0:39:02.\n",
      "  Batch 4,720  of  7,605.    Elapsed: 0:39:22.\n",
      "  Batch 4,760  of  7,605.    Elapsed: 0:39:42.\n",
      "  Batch 4,800  of  7,605.    Elapsed: 0:40:02.\n",
      "  Batch 4,840  of  7,605.    Elapsed: 0:40:22.\n",
      "  Batch 4,880  of  7,605.    Elapsed: 0:40:42.\n",
      "  Batch 4,920  of  7,605.    Elapsed: 0:41:02.\n",
      "  Batch 4,960  of  7,605.    Elapsed: 0:41:22.\n",
      "  Batch 5,000  of  7,605.    Elapsed: 0:41:42.\n",
      "  Batch 5,040  of  7,605.    Elapsed: 0:42:02.\n",
      "  Batch 5,080  of  7,605.    Elapsed: 0:42:22.\n",
      "  Batch 5,120  of  7,605.    Elapsed: 0:42:42.\n",
      "  Batch 5,160  of  7,605.    Elapsed: 0:43:02.\n",
      "  Batch 5,200  of  7,605.    Elapsed: 0:43:22.\n",
      "  Batch 5,240  of  7,605.    Elapsed: 0:43:42.\n",
      "  Batch 5,280  of  7,605.    Elapsed: 0:44:03.\n",
      "  Batch 5,320  of  7,605.    Elapsed: 0:44:23.\n",
      "  Batch 5,360  of  7,605.    Elapsed: 0:44:43.\n",
      "  Batch 5,400  of  7,605.    Elapsed: 0:45:03.\n",
      "  Batch 5,440  of  7,605.    Elapsed: 0:45:23.\n",
      "  Batch 5,480  of  7,605.    Elapsed: 0:45:43.\n",
      "  Batch 5,520  of  7,605.    Elapsed: 0:46:03.\n",
      "  Batch 5,560  of  7,605.    Elapsed: 0:46:23.\n",
      "  Batch 5,600  of  7,605.    Elapsed: 0:46:43.\n",
      "  Batch 5,640  of  7,605.    Elapsed: 0:47:03.\n",
      "  Batch 5,680  of  7,605.    Elapsed: 0:47:23.\n",
      "  Batch 5,720  of  7,605.    Elapsed: 0:47:43.\n",
      "  Batch 5,760  of  7,605.    Elapsed: 0:48:03.\n",
      "  Batch 5,800  of  7,605.    Elapsed: 0:48:23.\n",
      "  Batch 5,840  of  7,605.    Elapsed: 0:48:43.\n",
      "  Batch 5,880  of  7,605.    Elapsed: 0:49:03.\n",
      "  Batch 5,920  of  7,605.    Elapsed: 0:49:23.\n",
      "  Batch 5,960  of  7,605.    Elapsed: 0:49:43.\n",
      "  Batch 6,000  of  7,605.    Elapsed: 0:50:03.\n",
      "  Batch 6,040  of  7,605.    Elapsed: 0:50:23.\n",
      "  Batch 6,080  of  7,605.    Elapsed: 0:50:43.\n",
      "  Batch 6,120  of  7,605.    Elapsed: 0:51:03.\n",
      "  Batch 6,160  of  7,605.    Elapsed: 0:51:23.\n",
      "  Batch 6,200  of  7,605.    Elapsed: 0:51:43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 6,240  of  7,605.    Elapsed: 0:52:03.\n",
      "  Batch 6,280  of  7,605.    Elapsed: 0:52:23.\n",
      "  Batch 6,320  of  7,605.    Elapsed: 0:52:43.\n",
      "  Batch 6,360  of  7,605.    Elapsed: 0:53:03.\n",
      "  Batch 6,400  of  7,605.    Elapsed: 0:53:23.\n",
      "  Batch 6,440  of  7,605.    Elapsed: 0:53:43.\n",
      "  Batch 6,480  of  7,605.    Elapsed: 0:54:03.\n",
      "  Batch 6,520  of  7,605.    Elapsed: 0:54:23.\n",
      "  Batch 6,560  of  7,605.    Elapsed: 0:54:43.\n",
      "  Batch 6,600  of  7,605.    Elapsed: 0:55:03.\n",
      "  Batch 6,640  of  7,605.    Elapsed: 0:55:23.\n",
      "  Batch 6,680  of  7,605.    Elapsed: 0:55:43.\n",
      "  Batch 6,720  of  7,605.    Elapsed: 0:56:03.\n",
      "  Batch 6,760  of  7,605.    Elapsed: 0:56:23.\n",
      "  Batch 6,800  of  7,605.    Elapsed: 0:56:43.\n",
      "  Batch 6,840  of  7,605.    Elapsed: 0:57:03.\n",
      "  Batch 6,880  of  7,605.    Elapsed: 0:57:23.\n",
      "  Batch 6,920  of  7,605.    Elapsed: 0:57:43.\n",
      "  Batch 6,960  of  7,605.    Elapsed: 0:58:03.\n",
      "  Batch 7,000  of  7,605.    Elapsed: 0:58:23.\n",
      "  Batch 7,040  of  7,605.    Elapsed: 0:58:43.\n",
      "  Batch 7,080  of  7,605.    Elapsed: 0:59:04.\n",
      "  Batch 7,120  of  7,605.    Elapsed: 0:59:24.\n",
      "  Batch 7,160  of  7,605.    Elapsed: 0:59:44.\n",
      "  Batch 7,200  of  7,605.    Elapsed: 1:00:04.\n",
      "  Batch 7,240  of  7,605.    Elapsed: 1:00:24.\n",
      "  Batch 7,280  of  7,605.    Elapsed: 1:00:44.\n",
      "  Batch 7,320  of  7,605.    Elapsed: 1:01:04.\n",
      "  Batch 7,360  of  7,605.    Elapsed: 1:01:24.\n",
      "  Batch 7,400  of  7,605.    Elapsed: 1:01:44.\n",
      "  Batch 7,440  of  7,605.    Elapsed: 1:02:04.\n",
      "  Batch 7,480  of  7,605.    Elapsed: 1:02:24.\n",
      "  Batch 7,520  of  7,605.    Elapsed: 1:02:44.\n",
      "  Batch 7,560  of  7,605.    Elapsed: 1:03:04.\n",
      "  Batch 7,600  of  7,605.    Elapsed: 1:03:24.\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epcoh took: 1:03:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60\n",
      "  Validation Loss: 0.66\n",
      "  Validation took: 0:14:53\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,605.    Elapsed: 0:00:20.\n",
      "  Batch    80  of  7,605.    Elapsed: 0:00:40.\n",
      "  Batch   120  of  7,605.    Elapsed: 0:01:00.\n",
      "  Batch   160  of  7,605.    Elapsed: 0:01:20.\n",
      "  Batch   200  of  7,605.    Elapsed: 0:01:40.\n",
      "  Batch   240  of  7,605.    Elapsed: 0:02:00.\n",
      "  Batch   280  of  7,605.    Elapsed: 0:02:20.\n",
      "  Batch   320  of  7,605.    Elapsed: 0:02:40.\n",
      "  Batch   360  of  7,605.    Elapsed: 0:03:00.\n",
      "  Batch   400  of  7,605.    Elapsed: 0:03:20.\n",
      "  Batch   440  of  7,605.    Elapsed: 0:03:40.\n",
      "  Batch   480  of  7,605.    Elapsed: 0:04:00.\n",
      "  Batch   520  of  7,605.    Elapsed: 0:04:20.\n",
      "  Batch   560  of  7,605.    Elapsed: 0:04:40.\n",
      "  Batch   600  of  7,605.    Elapsed: 0:05:00.\n",
      "  Batch   640  of  7,605.    Elapsed: 0:05:20.\n",
      "  Batch   680  of  7,605.    Elapsed: 0:05:40.\n",
      "  Batch   720  of  7,605.    Elapsed: 0:06:00.\n",
      "  Batch   760  of  7,605.    Elapsed: 0:06:20.\n",
      "  Batch   800  of  7,605.    Elapsed: 0:06:40.\n",
      "  Batch   840  of  7,605.    Elapsed: 0:07:01.\n",
      "  Batch   880  of  7,605.    Elapsed: 0:07:21.\n",
      "  Batch   920  of  7,605.    Elapsed: 0:07:41.\n",
      "  Batch   960  of  7,605.    Elapsed: 0:08:01.\n",
      "  Batch 1,000  of  7,605.    Elapsed: 0:08:21.\n",
      "  Batch 1,040  of  7,605.    Elapsed: 0:08:41.\n",
      "  Batch 1,080  of  7,605.    Elapsed: 0:09:01.\n",
      "  Batch 1,120  of  7,605.    Elapsed: 0:09:21.\n",
      "  Batch 1,160  of  7,605.    Elapsed: 0:09:41.\n",
      "  Batch 1,200  of  7,605.    Elapsed: 0:10:01.\n",
      "  Batch 1,240  of  7,605.    Elapsed: 0:10:21.\n",
      "  Batch 1,280  of  7,605.    Elapsed: 0:10:41.\n",
      "  Batch 1,320  of  7,605.    Elapsed: 0:11:01.\n",
      "  Batch 1,360  of  7,605.    Elapsed: 0:11:21.\n",
      "  Batch 1,400  of  7,605.    Elapsed: 0:11:41.\n",
      "  Batch 1,440  of  7,605.    Elapsed: 0:12:01.\n",
      "  Batch 1,480  of  7,605.    Elapsed: 0:12:21.\n",
      "  Batch 1,520  of  7,605.    Elapsed: 0:12:41.\n",
      "  Batch 1,560  of  7,605.    Elapsed: 0:13:01.\n",
      "  Batch 1,600  of  7,605.    Elapsed: 0:13:21.\n",
      "  Batch 1,640  of  7,605.    Elapsed: 0:13:41.\n",
      "  Batch 1,680  of  7,605.    Elapsed: 0:14:01.\n",
      "  Batch 1,720  of  7,605.    Elapsed: 0:14:21.\n",
      "  Batch 1,760  of  7,605.    Elapsed: 0:14:41.\n",
      "  Batch 1,800  of  7,605.    Elapsed: 0:15:01.\n",
      "  Batch 1,840  of  7,605.    Elapsed: 0:15:21.\n",
      "  Batch 1,880  of  7,605.    Elapsed: 0:15:41.\n",
      "  Batch 1,920  of  7,605.    Elapsed: 0:16:01.\n",
      "  Batch 1,960  of  7,605.    Elapsed: 0:16:21.\n",
      "  Batch 2,000  of  7,605.    Elapsed: 0:16:41.\n",
      "  Batch 2,040  of  7,605.    Elapsed: 0:17:01.\n",
      "  Batch 2,080  of  7,605.    Elapsed: 0:17:21.\n",
      "  Batch 2,120  of  7,605.    Elapsed: 0:17:41.\n",
      "  Batch 2,160  of  7,605.    Elapsed: 0:18:01.\n",
      "  Batch 2,200  of  7,605.    Elapsed: 0:18:21.\n",
      "  Batch 2,240  of  7,605.    Elapsed: 0:18:41.\n",
      "  Batch 2,280  of  7,605.    Elapsed: 0:19:01.\n",
      "  Batch 2,320  of  7,605.    Elapsed: 0:19:21.\n",
      "  Batch 2,360  of  7,605.    Elapsed: 0:19:41.\n",
      "  Batch 2,400  of  7,605.    Elapsed: 0:20:01.\n",
      "  Batch 2,440  of  7,605.    Elapsed: 0:20:21.\n",
      "  Batch 2,480  of  7,605.    Elapsed: 0:20:41.\n",
      "  Batch 2,520  of  7,605.    Elapsed: 0:21:01.\n",
      "  Batch 2,560  of  7,605.    Elapsed: 0:21:21.\n",
      "  Batch 2,600  of  7,605.    Elapsed: 0:21:41.\n",
      "  Batch 2,640  of  7,605.    Elapsed: 0:22:01.\n",
      "  Batch 2,680  of  7,605.    Elapsed: 0:22:21.\n",
      "  Batch 2,720  of  7,605.    Elapsed: 0:22:41.\n",
      "  Batch 2,760  of  7,605.    Elapsed: 0:23:01.\n",
      "  Batch 2,800  of  7,605.    Elapsed: 0:23:21.\n",
      "  Batch 2,840  of  7,605.    Elapsed: 0:23:42.\n",
      "  Batch 2,880  of  7,605.    Elapsed: 0:24:02.\n",
      "  Batch 2,920  of  7,605.    Elapsed: 0:24:22.\n",
      "  Batch 2,960  of  7,605.    Elapsed: 0:24:42.\n",
      "  Batch 3,000  of  7,605.    Elapsed: 0:25:02.\n",
      "  Batch 3,040  of  7,605.    Elapsed: 0:25:22.\n",
      "  Batch 3,080  of  7,605.    Elapsed: 0:25:42.\n",
      "  Batch 3,120  of  7,605.    Elapsed: 0:26:02.\n",
      "  Batch 3,160  of  7,605.    Elapsed: 0:26:22.\n",
      "  Batch 3,200  of  7,605.    Elapsed: 0:26:42.\n",
      "  Batch 3,240  of  7,605.    Elapsed: 0:27:02.\n",
      "  Batch 3,280  of  7,605.    Elapsed: 0:27:22.\n",
      "  Batch 3,320  of  7,605.    Elapsed: 0:27:42.\n",
      "  Batch 3,360  of  7,605.    Elapsed: 0:28:02.\n",
      "  Batch 3,400  of  7,605.    Elapsed: 0:28:22.\n",
      "  Batch 3,440  of  7,605.    Elapsed: 0:28:42.\n",
      "  Batch 3,480  of  7,605.    Elapsed: 0:29:02.\n",
      "  Batch 3,520  of  7,605.    Elapsed: 0:29:22.\n",
      "  Batch 3,560  of  7,605.    Elapsed: 0:29:42.\n",
      "  Batch 3,600  of  7,605.    Elapsed: 0:30:02.\n",
      "  Batch 3,640  of  7,605.    Elapsed: 0:30:22.\n",
      "  Batch 3,680  of  7,605.    Elapsed: 0:30:42.\n",
      "  Batch 3,720  of  7,605.    Elapsed: 0:31:02.\n",
      "  Batch 3,760  of  7,605.    Elapsed: 0:31:22.\n",
      "  Batch 3,800  of  7,605.    Elapsed: 0:31:42.\n",
      "  Batch 3,840  of  7,605.    Elapsed: 0:32:02.\n",
      "  Batch 3,880  of  7,605.    Elapsed: 0:32:22.\n",
      "  Batch 3,920  of  7,605.    Elapsed: 0:32:42.\n",
      "  Batch 3,960  of  7,605.    Elapsed: 0:33:02.\n",
      "  Batch 4,000  of  7,605.    Elapsed: 0:33:22.\n",
      "  Batch 4,040  of  7,605.    Elapsed: 0:33:42.\n",
      "  Batch 4,080  of  7,605.    Elapsed: 0:34:02.\n",
      "  Batch 4,120  of  7,605.    Elapsed: 0:34:22.\n",
      "  Batch 4,160  of  7,605.    Elapsed: 0:34:42.\n",
      "  Batch 4,200  of  7,605.    Elapsed: 0:35:02.\n",
      "  Batch 4,240  of  7,605.    Elapsed: 0:35:22.\n",
      "  Batch 4,280  of  7,605.    Elapsed: 0:35:42.\n",
      "  Batch 4,320  of  7,605.    Elapsed: 0:36:02.\n",
      "  Batch 4,360  of  7,605.    Elapsed: 0:36:22.\n",
      "  Batch 4,400  of  7,605.    Elapsed: 0:36:42.\n",
      "  Batch 4,440  of  7,605.    Elapsed: 0:37:02.\n",
      "  Batch 4,480  of  7,605.    Elapsed: 0:37:22.\n",
      "  Batch 4,520  of  7,605.    Elapsed: 0:37:42.\n",
      "  Batch 4,560  of  7,605.    Elapsed: 0:38:02.\n",
      "  Batch 4,600  of  7,605.    Elapsed: 0:38:22.\n",
      "  Batch 4,640  of  7,605.    Elapsed: 0:38:42.\n",
      "  Batch 4,680  of  7,605.    Elapsed: 0:39:02.\n",
      "  Batch 4,720  of  7,605.    Elapsed: 0:39:22.\n",
      "  Batch 4,760  of  7,605.    Elapsed: 0:39:42.\n",
      "  Batch 4,800  of  7,605.    Elapsed: 0:40:02.\n",
      "  Batch 4,840  of  7,605.    Elapsed: 0:40:22.\n",
      "  Batch 4,880  of  7,605.    Elapsed: 0:40:42.\n",
      "  Batch 4,920  of  7,605.    Elapsed: 0:41:02.\n",
      "  Batch 4,960  of  7,605.    Elapsed: 0:41:22.\n",
      "  Batch 5,000  of  7,605.    Elapsed: 0:41:42.\n",
      "  Batch 5,040  of  7,605.    Elapsed: 0:42:02.\n",
      "  Batch 5,080  of  7,605.    Elapsed: 0:42:22.\n",
      "  Batch 5,120  of  7,605.    Elapsed: 0:42:42.\n",
      "  Batch 5,160  of  7,605.    Elapsed: 0:43:02.\n",
      "  Batch 5,200  of  7,605.    Elapsed: 0:43:22.\n",
      "  Batch 5,240  of  7,605.    Elapsed: 0:43:42.\n",
      "  Batch 5,280  of  7,605.    Elapsed: 0:44:02.\n",
      "  Batch 5,320  of  7,605.    Elapsed: 0:44:22.\n",
      "  Batch 5,360  of  7,605.    Elapsed: 0:44:42.\n",
      "  Batch 5,400  of  7,605.    Elapsed: 0:45:02.\n",
      "  Batch 5,440  of  7,605.    Elapsed: 0:45:22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,480  of  7,605.    Elapsed: 0:45:42.\n",
      "  Batch 5,520  of  7,605.    Elapsed: 0:46:02.\n",
      "  Batch 5,560  of  7,605.    Elapsed: 0:46:22.\n",
      "  Batch 5,600  of  7,605.    Elapsed: 0:46:42.\n",
      "  Batch 5,640  of  7,605.    Elapsed: 0:47:02.\n",
      "  Batch 5,680  of  7,605.    Elapsed: 0:47:22.\n",
      "  Batch 5,720  of  7,605.    Elapsed: 0:47:42.\n",
      "  Batch 5,760  of  7,605.    Elapsed: 0:48:02.\n",
      "  Batch 5,800  of  7,605.    Elapsed: 0:48:23.\n",
      "  Batch 5,840  of  7,605.    Elapsed: 0:48:43.\n",
      "  Batch 5,880  of  7,605.    Elapsed: 0:49:03.\n",
      "  Batch 5,920  of  7,605.    Elapsed: 0:49:23.\n",
      "  Batch 5,960  of  7,605.    Elapsed: 0:49:43.\n",
      "  Batch 6,000  of  7,605.    Elapsed: 0:50:03.\n",
      "  Batch 6,040  of  7,605.    Elapsed: 0:50:23.\n",
      "  Batch 6,080  of  7,605.    Elapsed: 0:50:43.\n",
      "  Batch 6,120  of  7,605.    Elapsed: 0:51:03.\n",
      "  Batch 6,160  of  7,605.    Elapsed: 0:51:23.\n",
      "  Batch 6,200  of  7,605.    Elapsed: 0:51:43.\n",
      "  Batch 6,240  of  7,605.    Elapsed: 0:52:03.\n",
      "  Batch 6,280  of  7,605.    Elapsed: 0:52:23.\n",
      "  Batch 6,320  of  7,605.    Elapsed: 0:52:43.\n",
      "  Batch 6,360  of  7,605.    Elapsed: 0:53:03.\n",
      "  Batch 6,400  of  7,605.    Elapsed: 0:53:23.\n",
      "  Batch 6,440  of  7,605.    Elapsed: 0:53:43.\n",
      "  Batch 6,480  of  7,605.    Elapsed: 0:54:03.\n",
      "  Batch 6,520  of  7,605.    Elapsed: 0:54:23.\n",
      "  Batch 6,560  of  7,605.    Elapsed: 0:54:43.\n",
      "  Batch 6,600  of  7,605.    Elapsed: 0:55:03.\n",
      "  Batch 6,640  of  7,605.    Elapsed: 0:55:23.\n",
      "  Batch 6,680  of  7,605.    Elapsed: 0:55:43.\n",
      "  Batch 6,720  of  7,605.    Elapsed: 0:56:03.\n",
      "  Batch 6,760  of  7,605.    Elapsed: 0:56:23.\n",
      "  Batch 6,800  of  7,605.    Elapsed: 0:56:43.\n",
      "  Batch 6,840  of  7,605.    Elapsed: 0:57:03.\n",
      "  Batch 6,880  of  7,605.    Elapsed: 0:57:23.\n",
      "  Batch 6,920  of  7,605.    Elapsed: 0:57:43.\n",
      "  Batch 6,960  of  7,605.    Elapsed: 0:58:03.\n",
      "  Batch 7,000  of  7,605.    Elapsed: 0:58:23.\n",
      "  Batch 7,040  of  7,605.    Elapsed: 0:58:43.\n",
      "  Batch 7,080  of  7,605.    Elapsed: 0:59:03.\n",
      "  Batch 7,120  of  7,605.    Elapsed: 0:59:23.\n",
      "  Batch 7,160  of  7,605.    Elapsed: 0:59:43.\n",
      "  Batch 7,200  of  7,605.    Elapsed: 1:00:03.\n",
      "  Batch 7,240  of  7,605.    Elapsed: 1:00:23.\n",
      "  Batch 7,280  of  7,605.    Elapsed: 1:00:43.\n",
      "  Batch 7,320  of  7,605.    Elapsed: 1:01:03.\n",
      "  Batch 7,360  of  7,605.    Elapsed: 1:01:23.\n",
      "  Batch 7,400  of  7,605.    Elapsed: 1:01:43.\n",
      "  Batch 7,440  of  7,605.    Elapsed: 1:02:03.\n",
      "  Batch 7,480  of  7,605.    Elapsed: 1:02:23.\n",
      "  Batch 7,520  of  7,605.    Elapsed: 1:02:43.\n",
      "  Batch 7,560  of  7,605.    Elapsed: 1:03:03.\n",
      "  Batch 7,600  of  7,605.    Elapsed: 1:03:23.\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epcoh took: 1:03:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60\n",
      "  Validation Loss: 0.66\n",
      "  Validation took: 0:14:52\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  7,605.    Elapsed: 0:00:20.\n",
      "  Batch    80  of  7,605.    Elapsed: 0:00:40.\n",
      "  Batch   120  of  7,605.    Elapsed: 0:01:00.\n",
      "  Batch   160  of  7,605.    Elapsed: 0:01:20.\n",
      "  Batch   200  of  7,605.    Elapsed: 0:01:40.\n",
      "  Batch   240  of  7,605.    Elapsed: 0:02:00.\n",
      "  Batch   280  of  7,605.    Elapsed: 0:02:20.\n",
      "  Batch   320  of  7,605.    Elapsed: 0:02:40.\n",
      "  Batch   360  of  7,605.    Elapsed: 0:03:00.\n",
      "  Batch   400  of  7,605.    Elapsed: 0:03:20.\n",
      "  Batch   440  of  7,605.    Elapsed: 0:03:40.\n",
      "  Batch   480  of  7,605.    Elapsed: 0:04:00.\n",
      "  Batch   520  of  7,605.    Elapsed: 0:04:20.\n",
      "  Batch   560  of  7,605.    Elapsed: 0:04:40.\n",
      "  Batch   600  of  7,605.    Elapsed: 0:05:00.\n",
      "  Batch   640  of  7,605.    Elapsed: 0:05:20.\n",
      "  Batch   680  of  7,605.    Elapsed: 0:05:40.\n",
      "  Batch   720  of  7,605.    Elapsed: 0:06:00.\n",
      "  Batch   760  of  7,605.    Elapsed: 0:06:20.\n",
      "  Batch   800  of  7,605.    Elapsed: 0:06:40.\n",
      "  Batch   840  of  7,605.    Elapsed: 0:07:00.\n",
      "  Batch   880  of  7,605.    Elapsed: 0:07:20.\n",
      "  Batch   920  of  7,605.    Elapsed: 0:07:40.\n",
      "  Batch   960  of  7,605.    Elapsed: 0:08:00.\n",
      "  Batch 1,000  of  7,605.    Elapsed: 0:08:20.\n",
      "  Batch 1,040  of  7,605.    Elapsed: 0:08:40.\n",
      "  Batch 1,080  of  7,605.    Elapsed: 0:09:00.\n",
      "  Batch 1,120  of  7,605.    Elapsed: 0:09:20.\n",
      "  Batch 1,160  of  7,605.    Elapsed: 0:09:40.\n",
      "  Batch 1,200  of  7,605.    Elapsed: 0:10:00.\n",
      "  Batch 1,240  of  7,605.    Elapsed: 0:10:21.\n",
      "  Batch 1,280  of  7,605.    Elapsed: 0:10:41.\n",
      "  Batch 1,320  of  7,605.    Elapsed: 0:11:01.\n",
      "  Batch 1,360  of  7,605.    Elapsed: 0:11:21.\n",
      "  Batch 1,400  of  7,605.    Elapsed: 0:11:41.\n",
      "  Batch 1,440  of  7,605.    Elapsed: 0:12:01.\n",
      "  Batch 1,480  of  7,605.    Elapsed: 0:12:21.\n",
      "  Batch 1,520  of  7,605.    Elapsed: 0:12:41.\n",
      "  Batch 1,560  of  7,605.    Elapsed: 0:13:01.\n",
      "  Batch 1,600  of  7,605.    Elapsed: 0:13:21.\n",
      "  Batch 1,640  of  7,605.    Elapsed: 0:13:41.\n",
      "  Batch 1,680  of  7,605.    Elapsed: 0:14:01.\n",
      "  Batch 1,720  of  7,605.    Elapsed: 0:14:21.\n",
      "  Batch 1,760  of  7,605.    Elapsed: 0:14:41.\n",
      "  Batch 1,800  of  7,605.    Elapsed: 0:15:01.\n",
      "  Batch 1,840  of  7,605.    Elapsed: 0:15:21.\n",
      "  Batch 1,880  of  7,605.    Elapsed: 0:15:41.\n",
      "  Batch 1,920  of  7,605.    Elapsed: 0:16:01.\n",
      "  Batch 1,960  of  7,605.    Elapsed: 0:16:21.\n",
      "  Batch 2,000  of  7,605.    Elapsed: 0:16:41.\n",
      "  Batch 2,040  of  7,605.    Elapsed: 0:17:01.\n",
      "  Batch 2,080  of  7,605.    Elapsed: 0:17:21.\n",
      "  Batch 2,120  of  7,605.    Elapsed: 0:17:41.\n",
      "  Batch 2,160  of  7,605.    Elapsed: 0:18:01.\n",
      "  Batch 2,200  of  7,605.    Elapsed: 0:18:21.\n",
      "  Batch 2,240  of  7,605.    Elapsed: 0:18:41.\n",
      "  Batch 2,280  of  7,605.    Elapsed: 0:19:01.\n",
      "  Batch 2,320  of  7,605.    Elapsed: 0:19:21.\n",
      "  Batch 2,360  of  7,605.    Elapsed: 0:19:41.\n",
      "  Batch 2,400  of  7,605.    Elapsed: 0:20:01.\n",
      "  Batch 2,440  of  7,605.    Elapsed: 0:20:21.\n",
      "  Batch 2,480  of  7,605.    Elapsed: 0:20:41.\n",
      "  Batch 2,520  of  7,605.    Elapsed: 0:21:01.\n",
      "  Batch 2,560  of  7,605.    Elapsed: 0:21:21.\n",
      "  Batch 2,600  of  7,605.    Elapsed: 0:21:41.\n",
      "  Batch 2,640  of  7,605.    Elapsed: 0:22:01.\n",
      "  Batch 2,680  of  7,605.    Elapsed: 0:22:21.\n",
      "  Batch 2,720  of  7,605.    Elapsed: 0:22:41.\n",
      "  Batch 2,760  of  7,605.    Elapsed: 0:23:01.\n",
      "  Batch 2,800  of  7,605.    Elapsed: 0:23:21.\n",
      "  Batch 2,840  of  7,605.    Elapsed: 0:23:41.\n",
      "  Batch 2,880  of  7,605.    Elapsed: 0:24:01.\n",
      "  Batch 2,920  of  7,605.    Elapsed: 0:24:21.\n",
      "  Batch 2,960  of  7,605.    Elapsed: 0:24:41.\n",
      "  Batch 3,000  of  7,605.    Elapsed: 0:25:01.\n",
      "  Batch 3,040  of  7,605.    Elapsed: 0:25:21.\n",
      "  Batch 3,080  of  7,605.    Elapsed: 0:25:41.\n",
      "  Batch 3,120  of  7,605.    Elapsed: 0:26:01.\n",
      "  Batch 3,160  of  7,605.    Elapsed: 0:26:21.\n",
      "  Batch 3,200  of  7,605.    Elapsed: 0:26:41.\n",
      "  Batch 3,240  of  7,605.    Elapsed: 0:27:01.\n",
      "  Batch 3,280  of  7,605.    Elapsed: 0:27:21.\n",
      "  Batch 3,320  of  7,605.    Elapsed: 0:27:41.\n",
      "  Batch 3,360  of  7,605.    Elapsed: 0:28:01.\n",
      "  Batch 3,400  of  7,605.    Elapsed: 0:28:21.\n",
      "  Batch 3,440  of  7,605.    Elapsed: 0:28:41.\n",
      "  Batch 3,480  of  7,605.    Elapsed: 0:29:01.\n",
      "  Batch 3,520  of  7,605.    Elapsed: 0:29:21.\n",
      "  Batch 3,560  of  7,605.    Elapsed: 0:29:41.\n",
      "  Batch 3,600  of  7,605.    Elapsed: 0:30:01.\n",
      "  Batch 3,640  of  7,605.    Elapsed: 0:30:22.\n",
      "  Batch 3,680  of  7,605.    Elapsed: 0:30:42.\n",
      "  Batch 3,720  of  7,605.    Elapsed: 0:31:02.\n",
      "  Batch 3,760  of  7,605.    Elapsed: 0:31:22.\n",
      "  Batch 3,800  of  7,605.    Elapsed: 0:31:42.\n",
      "  Batch 3,840  of  7,605.    Elapsed: 0:32:02.\n",
      "  Batch 3,880  of  7,605.    Elapsed: 0:32:22.\n",
      "  Batch 3,920  of  7,605.    Elapsed: 0:32:42.\n",
      "  Batch 3,960  of  7,605.    Elapsed: 0:33:02.\n",
      "  Batch 4,000  of  7,605.    Elapsed: 0:33:22.\n",
      "  Batch 4,040  of  7,605.    Elapsed: 0:33:42.\n",
      "  Batch 4,080  of  7,605.    Elapsed: 0:34:02.\n",
      "  Batch 4,120  of  7,605.    Elapsed: 0:34:22.\n",
      "  Batch 4,160  of  7,605.    Elapsed: 0:34:42.\n",
      "  Batch 4,200  of  7,605.    Elapsed: 0:35:02.\n",
      "  Batch 4,240  of  7,605.    Elapsed: 0:35:22.\n",
      "  Batch 4,280  of  7,605.    Elapsed: 0:35:42.\n",
      "  Batch 4,320  of  7,605.    Elapsed: 0:36:02.\n",
      "  Batch 4,360  of  7,605.    Elapsed: 0:36:22.\n",
      "  Batch 4,400  of  7,605.    Elapsed: 0:36:42.\n",
      "  Batch 4,440  of  7,605.    Elapsed: 0:37:02.\n",
      "  Batch 4,480  of  7,605.    Elapsed: 0:37:22.\n",
      "  Batch 4,520  of  7,605.    Elapsed: 0:37:42.\n",
      "  Batch 4,560  of  7,605.    Elapsed: 0:38:02.\n",
      "  Batch 4,600  of  7,605.    Elapsed: 0:38:22.\n",
      "  Batch 4,640  of  7,605.    Elapsed: 0:38:42.\n",
      "  Batch 4,680  of  7,605.    Elapsed: 0:39:02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4,720  of  7,605.    Elapsed: 0:39:22.\n",
      "  Batch 4,760  of  7,605.    Elapsed: 0:39:42.\n",
      "  Batch 4,800  of  7,605.    Elapsed: 0:40:02.\n",
      "  Batch 4,840  of  7,605.    Elapsed: 0:40:22.\n",
      "  Batch 4,880  of  7,605.    Elapsed: 0:40:42.\n",
      "  Batch 4,920  of  7,605.    Elapsed: 0:41:02.\n",
      "  Batch 4,960  of  7,605.    Elapsed: 0:41:22.\n",
      "  Batch 5,000  of  7,605.    Elapsed: 0:41:42.\n",
      "  Batch 5,040  of  7,605.    Elapsed: 0:42:02.\n",
      "  Batch 5,080  of  7,605.    Elapsed: 0:42:22.\n",
      "  Batch 5,120  of  7,605.    Elapsed: 0:42:42.\n",
      "  Batch 5,160  of  7,605.    Elapsed: 0:43:02.\n",
      "  Batch 5,200  of  7,605.    Elapsed: 0:43:22.\n",
      "  Batch 5,240  of  7,605.    Elapsed: 0:43:42.\n",
      "  Batch 5,280  of  7,605.    Elapsed: 0:44:02.\n",
      "  Batch 5,320  of  7,605.    Elapsed: 0:44:22.\n",
      "  Batch 5,360  of  7,605.    Elapsed: 0:44:42.\n",
      "  Batch 5,400  of  7,605.    Elapsed: 0:45:02.\n",
      "  Batch 5,440  of  7,605.    Elapsed: 0:45:22.\n",
      "  Batch 5,480  of  7,605.    Elapsed: 0:45:42.\n",
      "  Batch 5,520  of  7,605.    Elapsed: 0:46:02.\n",
      "  Batch 5,560  of  7,605.    Elapsed: 0:46:22.\n",
      "  Batch 5,600  of  7,605.    Elapsed: 0:46:42.\n",
      "  Batch 5,640  of  7,605.    Elapsed: 0:47:02.\n",
      "  Batch 5,680  of  7,605.    Elapsed: 0:47:22.\n",
      "  Batch 5,720  of  7,605.    Elapsed: 0:47:42.\n",
      "  Batch 5,760  of  7,605.    Elapsed: 0:48:02.\n",
      "  Batch 5,800  of  7,605.    Elapsed: 0:48:22.\n",
      "  Batch 5,840  of  7,605.    Elapsed: 0:48:42.\n",
      "  Batch 5,880  of  7,605.    Elapsed: 0:49:02.\n",
      "  Batch 5,920  of  7,605.    Elapsed: 0:49:22.\n",
      "  Batch 5,960  of  7,605.    Elapsed: 0:49:42.\n",
      "  Batch 6,000  of  7,605.    Elapsed: 0:50:02.\n",
      "  Batch 6,040  of  7,605.    Elapsed: 0:50:22.\n",
      "  Batch 6,080  of  7,605.    Elapsed: 0:50:42.\n",
      "  Batch 6,120  of  7,605.    Elapsed: 0:51:02.\n",
      "  Batch 6,160  of  7,605.    Elapsed: 0:51:22.\n",
      "  Batch 6,200  of  7,605.    Elapsed: 0:51:42.\n",
      "  Batch 6,240  of  7,605.    Elapsed: 0:52:02.\n",
      "  Batch 6,280  of  7,605.    Elapsed: 0:52:22.\n",
      "  Batch 6,320  of  7,605.    Elapsed: 0:52:42.\n",
      "  Batch 6,360  of  7,605.    Elapsed: 0:53:02.\n",
      "  Batch 6,400  of  7,605.    Elapsed: 0:53:22.\n",
      "  Batch 6,440  of  7,605.    Elapsed: 0:53:42.\n",
      "  Batch 6,480  of  7,605.    Elapsed: 0:54:02.\n",
      "  Batch 6,520  of  7,605.    Elapsed: 0:54:22.\n",
      "  Batch 6,560  of  7,605.    Elapsed: 0:54:42.\n",
      "  Batch 6,600  of  7,605.    Elapsed: 0:55:02.\n",
      "  Batch 6,640  of  7,605.    Elapsed: 0:55:22.\n",
      "  Batch 6,680  of  7,605.    Elapsed: 0:55:42.\n",
      "  Batch 6,720  of  7,605.    Elapsed: 0:56:02.\n",
      "  Batch 6,760  of  7,605.    Elapsed: 0:56:22.\n",
      "  Batch 6,800  of  7,605.    Elapsed: 0:56:43.\n",
      "  Batch 6,840  of  7,605.    Elapsed: 0:57:03.\n",
      "  Batch 6,880  of  7,605.    Elapsed: 0:57:23.\n",
      "  Batch 6,920  of  7,605.    Elapsed: 0:57:43.\n",
      "  Batch 6,960  of  7,605.    Elapsed: 0:58:03.\n",
      "  Batch 7,000  of  7,605.    Elapsed: 0:58:23.\n",
      "  Batch 7,040  of  7,605.    Elapsed: 0:58:43.\n",
      "  Batch 7,080  of  7,605.    Elapsed: 0:59:03.\n",
      "  Batch 7,120  of  7,605.    Elapsed: 0:59:23.\n",
      "  Batch 7,160  of  7,605.    Elapsed: 0:59:43.\n",
      "  Batch 7,200  of  7,605.    Elapsed: 1:00:03.\n",
      "  Batch 7,240  of  7,605.    Elapsed: 1:00:23.\n",
      "  Batch 7,280  of  7,605.    Elapsed: 1:00:43.\n",
      "  Batch 7,320  of  7,605.    Elapsed: 1:01:03.\n",
      "  Batch 7,360  of  7,605.    Elapsed: 1:01:23.\n",
      "  Batch 7,400  of  7,605.    Elapsed: 1:01:43.\n",
      "  Batch 7,440  of  7,605.    Elapsed: 1:02:03.\n",
      "  Batch 7,480  of  7,605.    Elapsed: 1:02:23.\n",
      "  Batch 7,520  of  7,605.    Elapsed: 1:02:43.\n",
      "  Batch 7,560  of  7,605.    Elapsed: 1:03:03.\n",
      "  Batch 7,600  of  7,605.    Elapsed: 1:03:23.\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epcoh took: 1:03:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60\n",
      "  Validation Loss: 0.66\n",
      "  Validation took: 0:14:52\n",
      "\n",
      "Training complete!\n",
      "Total training took 5:13:08 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "train(model, epochs=4, batch_size=16, optimizer=optimizer, train_dataset=train_dataset, validation_dataset=test_dataset, use_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the path you wish to save the Attention model to: model_3_acc_60\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "1. BertForSequenceClassification (num_labels=2) basic layers frozen. Validation accuracy 0.6.\n",
    "2. BertForSequenceClassification (num_labels=2) basic layers not frozen. MEMORY SHORTAGE\n",
    "3. BertForNextSentencePrediction basic layers not frozen. Validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  4489,  ...,     0,     0,     0],\n",
       "         [  101,  2138,  1996,  ...,     0,     0,     0],\n",
       "         [  101,  1006,  1037,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2490,  2005,  ...,     0,     0,     0],\n",
       "         [  101, 23089, 22773,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  5675,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffe601658164ca9984ff44c817971a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cdd75aa49a464bae1dc889e7a1e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6875601806640625, 'learning_rate': 5e-05, 'epoch': 0.2628811777076761, 'total_flos': 10762693312512000, 'step': 500}\n",
      "{'loss': 0.671542236328125, 'learning_rate': 4.648283624085538e-05, 'epoch': 0.5257623554153522, 'total_flos': 21525386625024000, 'step': 1000}\n",
      "{'loss': 0.6713079833984374, 'learning_rate': 4.296567248171075e-05, 'epoch': 0.7886435331230284, 'total_flos': 32288079937536000, 'step': 1500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b839fcaafb469f859bed837f51f0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6677021484375, 'learning_rate': 3.9448508722566127e-05, 'epoch': 1.0515247108307044, 'total_flos': 43033956541747200, 'step': 2000}\n",
      "{'loss': 0.66580810546875, 'learning_rate': 3.59313449634215e-05, 'epoch': 1.3144058885383807, 'total_flos': 53796649854259200, 'step': 2500}\n",
      "{'loss': 0.66675439453125, 'learning_rate': 3.241418120427687e-05, 'epoch': 1.5772870662460567, 'total_flos': 64559343166771200, 'step': 3000}\n",
      "{'loss': 0.665990478515625, 'learning_rate': 2.8897017445132247e-05, 'epoch': 1.840168243953733, 'total_flos': 75322036479283200, 'step': 3500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0181f0b3900142b180122d44c142f73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.66303662109375, 'learning_rate': 2.537985368598762e-05, 'epoch': 2.103049421661409, 'total_flos': 86067913083494400, 'step': 4000}\n",
      "{'loss': 0.6658017578125, 'learning_rate': 2.1862689926842993e-05, 'epoch': 2.365930599369085, 'total_flos': 96830606396006400, 'step': 4500}\n",
      "{'loss': 0.66045068359375, 'learning_rate': 1.8345526167698368e-05, 'epoch': 2.6288117770767614, 'total_flos': 107593299708518400, 'step': 5000}\n",
      "{'loss': 0.66278076171875, 'learning_rate': 1.4828362408553741e-05, 'epoch': 2.891692954784437, 'total_flos': 118355993021030400, 'step': 5500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e6c840b6944b9ab83f5e411540e8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.66191162109375, 'learning_rate': 1.1311198649409118e-05, 'epoch': 3.1545741324921135, 'total_flos': 129101869625241600, 'step': 6000}\n",
      "{'loss': 0.66073193359375, 'learning_rate': 7.79403489026449e-06, 'epoch': 3.4174553101997898, 'total_flos': 139864562937753600, 'step': 6500}\n",
      "{'loss': 0.6628544921875, 'learning_rate': 4.276871131119865e-06, 'epoch': 3.680336487907466, 'total_flos': 150627256250265600, 'step': 7000}\n",
      "{'loss': 0.6613369140625, 'learning_rate': 7.597073719752392e-07, 'epoch': 3.943217665615142, 'total_flos': 161389949562777600, 'step': 7500}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DIRECTORY_NAME+'/BERT_results_final',          # output directory\n",
    "    num_train_epochs=4,              # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=DIRECTORY_NAME+'/BERT_logs_final',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(DIRECTORY_NAME+'/BERT_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4374da5c833d4833b4cf5dd56eb722b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478eb3d42e014062a6f191f90d6d77f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1addbc03a622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIRECTORY_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/BERT_model_final_larger_lr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    775\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DIRECTORY_NAME+'/BERT_results_final_larger_lr',          # output directory\n",
    "    num_train_epochs=4,              # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    learning_rate=0.001,\n",
    "    logging_dir=DIRECTORY_NAME+'/BERT_logs_final_larger_lr',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(DIRECTORY_NAME+'/BERT_model_final_larger_lr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b629651172ca44a19851c40493b91c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203df4a1937d4636b9fae49928ec9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6672554931640625, 'learning_rate': 3.6855941114616195e-05, 'epoch': 0.2628811777076761, 'total_flos': 10762693312512000, 'step': 500}\n",
      "{'loss': 0.665552490234375, 'learning_rate': 2.3711882229232387e-05, 'epoch': 0.5257623554153522, 'total_flos': 21525386625024000, 'step': 1000}\n",
      "{'loss': 0.664870361328125, 'learning_rate': 1.056782334384858e-05, 'epoch': 0.7886435331230284, 'total_flos': 32288079937536000, 'step': 1500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9376e57f2d64433a4611f9867510e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=476.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.6630513711166883, 'epoch': 1.0, 'total_flos': 40924468652494848, 'step': 1902}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "pt_model = BertForSequenceClassification.from_pretrained(\"/home/lovhag/projects/dl4nlp_assignment_1/BERT_results/BERT_model_1\")\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "for param in pt_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DIRECTORY_NAME+'/BERT_results_pt_1',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=0,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=DIRECTORY_NAME+'/BERT_logs_pt_1',            # directory for storing logs\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pt_model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(DIRECTORY_NAME+'/BERT_model_pt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192db1096b5b48de931b153496124a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd113017e3554087a0d41808d0df19a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1902.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b240b3008f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIRECTORY_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/BERT_model_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \"\"\"\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3_new/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "# keep the weights of the pre-trained encoder frozen and optimize only the weights of the head layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DIRECTORY_NAME+'/BERT_results_final',          # output directory\n",
    "    num_train_epochs=4,              # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=DIRECTORY_NAME+'/BERT_logs_final',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(DIRECTORY_NAME+'/BERT_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(DIRECTORY_NAME)\n",
    "tokenizer.save_pretrained(DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
