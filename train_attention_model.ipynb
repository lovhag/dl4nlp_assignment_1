{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Attention TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import help_functions\n",
    "import attention_models\n",
    "import better_attention_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRUCTURE\n",
    "3. Build the model with embedding and Attention.\n",
    "5. Train on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"saved_data/splitted_X_train_eq.pickle\")\n",
    "X_test = load_pickle_data(\"saved_data/splitted_X_test_eq.pickle\")\n",
    "y_train = load_pickle_data(\"saved_data/splitted_y_train_eq.pickle\")\n",
    "y_test = load_pickle_data(\"saved_data/splitted_y_test_eq.pickle\")\n",
    "voc = load_pickle_data(\"saved_data/all_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'attention_data/splitted_with_mask_X_train.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62a2e32bc316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_data/splitted_with_mask_X_train.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_data/splitted_with_mask_X_test.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_data/splitted_with_mask_y_train.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_data/splitted_with_mask_y_test.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_data/splitted_with_mask_mask_train.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bf893bd4e9ea>\u001b[0m in \u001b[0;36mload_pickle_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'attention_data/splitted_with_mask_X_train.pickle'"
     ]
    }
   ],
   "source": [
    "X_train = load_pickle_data(\"attention_data/splitted_with_mask_X_train.pickle\")\n",
    "X_test = load_pickle_data(\"attention_data/splitted_with_mask_X_test.pickle\")\n",
    "y_train = load_pickle_data(\"attention_data/splitted_with_mask_y_train.pickle\")\n",
    "y_test = load_pickle_data(\"attention_data/splitted_with_mask_y_test.pickle\")\n",
    "mask_train = load_pickle_data(\"attention_data/splitted_with_mask_mask_train.pickle\")\n",
    "mask_test = load_pickle_data(\"attention_data/splitted_with_mask_mask_test.pickle\")\n",
    "voc = load_pickle_data(\"attention_data/splitted_with_mask_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_X_train.pickle\")\n",
    "X_test = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_X_test.pickle\")\n",
    "y_train = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_y_train.pickle\")\n",
    "y_test = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_y_test.pickle\")\n",
    "mask_train = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_mask_train.pickle\")\n",
    "mask_test = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_mask_test.pickle\")\n",
    "voc = load_pickle_data(\"/home/lovhag/storage/data/attention_data/splitted_with_mask_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 101905\n",
      "Number of test samples: 50193\n",
      "\n",
      "Sequence length per sample: 283\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {len(y_train)}')\n",
    "print(f'Number of test samples: {len(y_test)}')\n",
    "print(\"\")\n",
    "print(f'Sequence length per sample: {max_sequence_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, batch_size, mask=None):\n",
    "    random_indices = torch.randperm(len(x))\n",
    "    for i in range(0, len(x) - batch_size + 1, batch_size):\n",
    "        indices = random_indices[i:i+batch_size]\n",
    "        if not type(mask) == type(None):\n",
    "            yield x[indices].to(device), y[indices].to(device), mask[indices].to(device)\n",
    "        else:\n",
    "            yield x[indices].to(device), y[indices].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by in batchify(X_train, y_train, batch_size):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(sample_weight)\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            accuracy = (output.eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by in batchify(X_test, y_test, batch_size):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mask(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5, padding_index=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by, bm in batchify(X_train, y_train, batch_size, mask_train):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            if padding_index is not None:\n",
    "                #print(\"using padding\")\n",
    "                padding_mask = (bx == padding_index)\n",
    "                #print(f\"padding_mask shape: {padding_mask.shape}\")\n",
    "                #print(f\"bx shape: {bx.shape}\")\n",
    "                #print(f\"padding_mask: {padding_mask}\")\n",
    "                output = model.forward(bx, bm, has_mask=True, padding_mask=padding_mask)\n",
    "            else:\n",
    "                #print(\"not using padding\")\n",
    "                output = model.forward(bx, bm)\n",
    "\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(\"Output after word position mask selection: \")\n",
    "            #print(torch.masked_select(bx, bm))\n",
    "            #print(sample_weight)\n",
    "            #print(f\"by shape: {by.shape}\")\n",
    "            #print(f\"output shape: {output.shape}\")\n",
    "            #print(f\"output: {output}\")\n",
    "            #print(f\"bx: {bx}\")\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor).to(device), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            #print(f\"output rounded: {output.round()}\")\n",
    "            #print(f\"by: {by}\")\n",
    "            accuracy = (output.round().eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by, bm in batchify(X_test, y_test, batch_size, mask_test):\n",
    "                nbr_test_batches += 1\n",
    "                if padding_index is not None:\n",
    "                    padding_mask = (bx == padding_index)\n",
    "                    output = model.forward(bx, bm, has_mask=True, padding_mask=padding_mask)\n",
    "                else:\n",
    "                    output = model.forward(bx, bm)\n",
    "\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor).to(device), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.round().eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_subset(sub_percentage, X_train, X_test, y_train, y_test):\n",
    "    train_sub_size = int(sub_percentage*len(y_train))\n",
    "    test_sub_size = int(sub_percentage*len(y_test))\n",
    "\n",
    "    X_train_sub = X_train[:train_sub_size]\n",
    "    X_test_sub = X_test[:test_sub_size]\n",
    "    y_train_sub = y_train[:train_sub_size]\n",
    "    y_test_sub = y_test[:test_sub_size]\n",
    "\n",
    "    return X_train_sub, X_test_sub, y_train_sub, y_test_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO TRY\n",
    "* Mask on word position.\n",
    "* Batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the path you wish to save the Attention model to: model_train_70_val_68\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload model modules if changes have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'better_attention_models' from '/home/lovhag/projects/dl4nlp_assignment_1/better_attention_models.py'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(attention_models)\n",
    "importlib.reload(better_attention_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PICKED MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013588581806933274 | train acc 0.4984983503818512 | validation loss 0.0013587181872156049 | validation acc 0.5009565949440002\n",
      "epoch 1 | train loss 0.0013550622248320124 | train acc 0.49988222122192383 | validation loss 0.0013551430887191044 | validation acc 0.49918287992477417\n",
      "epoch 2 | train loss 0.0013501058799453341 | train acc 0.510334849357605 | validation loss 0.001312868631021975 | validation acc 0.5499441623687744\n",
      "epoch 3 | train loss 0.0012949995457278987 | train acc 0.5478761196136475 | validation loss 0.0013036398093539234 | validation acc 0.5513392686843872\n",
      "epoch 4 | train loss 0.0012781836005306836 | train acc 0.565130352973938 | validation loss 0.001283921244939105 | validation acc 0.5756536722183228\n",
      "epoch 5 | train loss 0.0012489454516551302 | train acc 0.5910214781761169 | validation loss 0.0012754753697663546 | validation acc 0.5991709232330322\n",
      "epoch 6 | train loss 0.0011562844489738531 | train acc 0.6508028507232666 | validation loss 0.0012005581965964592 | validation acc 0.6614516973495483\n",
      "epoch 7 | train loss 0.001086239484940239 | train acc 0.6850070357322693 | validation loss 0.0011875799023166147 | validation acc 0.6763193607330322\n",
      "epoch 8 | train loss 0.0010595710679886386 | train acc 0.6955087780952454 | validation loss 0.0011604323407649348 | validation acc 0.6798867583274841\n",
      "epoch 9 | train loss 0.0010414291160057143 | train acc 0.7011817097663879 | validation loss 0.00117006032711028 | validation acc 0.683613657951355\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNormSkip(vocab_size=len(voc), embedding_dim=64, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013558863701378394 | train acc 0.4982725977897644 | validation loss 0.0013555329161154448 | validation acc 0.4967115521430969\n",
      "epoch 1 | train loss 0.0013542519273919676 | train acc 0.5008440613746643 | validation loss 0.0013545740541897487 | validation acc 0.49732938408851624\n",
      "epoch 2 | train loss 0.0013540163255956529 | train acc 0.5011875629425049 | validation loss 0.0013563185063076718 | validation acc 0.4972895383834839\n",
      "epoch 3 | train loss 0.0013500792123541123 | train acc 0.5109041333198547 | validation loss 0.001421582894589828 | validation acc 0.5523756146430969\n",
      "epoch 4 | train loss 0.0012829134869038236 | train acc 0.5666712522506714 | validation loss 0.0014096990213444342 | validation acc 0.5691167116165161\n",
      "epoch 5 | train loss 0.0012583805645357386 | train acc 0.5937696099281311 | validation loss 0.0014739972955015088 | validation acc 0.5896045565605164\n",
      "epoch 6 | train loss 0.0012077473814192056 | train acc 0.6318899393081665 | validation loss 0.0012150560060221398 | validation acc 0.6577048897743225\n",
      "epoch 7 | train loss 0.0011293360057881279 | train acc 0.6713057160377502 | validation loss 0.001251491026629751 | validation acc 0.6763591766357422\n",
      "epoch 8 | train loss 0.0011019288338767598 | train acc 0.6859591007232666 | validation loss 0.001221811782262212 | validation acc 0.680683970451355\n",
      "epoch 9 | train loss 0.0010861557124161522 | train acc 0.6910136342048645 | validation loss 0.0012239422078947633 | validation acc 0.6819196343421936\n",
      "epoch 10 | train loss 0.0010711842347591665 | train acc 0.6964019536972046 | validation loss 0.0012218508889010101 | validation acc 0.6817402839660645\n",
      "epoch 11 | train loss 0.0010607766185538497 | train acc 0.6991009712219238 | validation loss 0.0011890084694771627 | validation acc 0.6821189522743225\n",
      "epoch 12 | train loss 0.0010507334937486592 | train acc 0.701132595539093 | validation loss 0.0012416948385569932 | validation acc 0.6829161047935486\n",
      "epoch 13 | train loss 0.0010407582999162619 | train acc 0.7062264084815979 | validation loss 0.0012507786777117575 | validation acc 0.6827168464660645\n",
      "epoch 14 | train loss 0.0010305678971514257 | train acc 0.7071195840835571 | validation loss 0.0012745522271499646 | validation acc 0.6776148080825806\n",
      "epoch 15 | train loss 0.0010193424431277187 | train acc 0.7125176787376404 | validation loss 0.001272048089327766 | validation acc 0.6382932066917419\n",
      "epoch 16 | train loss 0.0010101825755555183 | train acc 0.7156779766082764 | validation loss 0.0012992840136724468 | validation acc 0.6382932066917419\n",
      "epoch 17 | train loss 0.0010006740783170408 | train acc 0.7218317985534668 | validation loss 0.0013375374451707288 | validation acc 0.637715220451355\n",
      "epoch 18 | train loss 0.000992632734714469 | train acc 0.7243443727493286 | validation loss 0.0013897677928646456 | validation acc 0.6224489808082581\n",
      "epoch 19 | train loss 0.0009850850198640203 | train acc 0.7277500629425049 | validation loss 0.001463999071904477 | validation acc 0.6356425285339355\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNormConcat(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=20, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013581157507534002 | train acc 0.4990283250808716 | validation loss 0.0013557819635322203 | validation acc 0.4989038407802582\n",
      "epoch 1 | train loss 0.001355243947979381 | train acc 0.4979192912578583 | validation loss 0.0013601954156836989 | validation acc 0.4976881146430969\n",
      "epoch 2 | train loss 0.0013366627039495462 | train acc 0.5223971605300903 | validation loss 0.0013006978831016363 | validation acc 0.5540098547935486\n",
      "epoch 3 | train loss 0.001290674906062552 | train acc 0.5528717637062073 | validation loss 0.0013385440837069206 | validation acc 0.5577766299247742\n",
      "epoch 4 | train loss 0.0012845441481992416 | train acc 0.5586133599281311 | validation loss 0.001309805164203922 | validation acc 0.5593112111091614\n",
      "epoch 5 | train loss 0.0012650645256683643 | train acc 0.5807749629020691 | validation loss 0.0012838503290015292 | validation acc 0.5852200388908386\n",
      "epoch 6 | train loss 0.001203375746233941 | train acc 0.6283860802650452 | validation loss 0.0012137176693483656 | validation acc 0.6598572731018066\n",
      "epoch 7 | train loss 0.0011235347903045783 | train acc 0.6736024022102356 | validation loss 0.0011864804647321223 | validation acc 0.6748644709587097\n",
      "epoch 8 | train loss 0.0010884279113475832 | train acc 0.6863320469856262 | validation loss 0.0011493126999548807 | validation acc 0.6798469424247742\n",
      "epoch 9 | train loss 0.0010659428384744718 | train acc 0.6934182643890381 | validation loss 0.001132016744623816 | validation acc 0.6796476244926453\n",
      "epoch 10 | train loss 0.001049430335809865 | train acc 0.6991304159164429 | validation loss 0.0011662231869306605 | validation acc 0.6819395422935486\n",
      "epoch 11 | train loss 0.0010344774666155721 | train acc 0.7030170559883118 | validation loss 0.0012032431823543596 | validation acc 0.679707407951355\n",
      "epoch 12 | train loss 0.0010215989363744817 | train acc 0.7060105204582214 | validation loss 0.0012801357272332913 | validation acc 0.6812619566917419\n",
      "epoch 13 | train loss 0.0010100193137429355 | train acc 0.7084445357322693 | validation loss 0.001316213206392808 | validation acc 0.6866828799247742\n",
      "epoch 14 | train loss 0.0009983462707406313 | train acc 0.7150890827178955 | validation loss 0.0013046612353087878 | validation acc 0.6901506781578064\n",
      "epoch 15 | train loss 0.0009857984360637396 | train acc 0.721910297870636 | validation loss 0.001407264841706207 | validation acc 0.6895129084587097\n",
      "epoch 16 | train loss 0.0009716902536295597 | train acc 0.7296344637870789 | validation loss 0.0013814582697315408 | validation acc 0.685945451259613\n",
      "epoch 17 | train loss 0.0009626716934905 | train acc 0.7321372628211975 | validation loss 0.0014701245360941226 | validation acc 0.6920838356018066\n",
      "epoch 18 | train loss 0.0009532049780912773 | train acc 0.7377512454986572 | validation loss 0.0015617417419870974 | validation acc 0.6893534660339355\n",
      "epoch 19 | train loss 0.0009424435474966002 | train acc 0.7412551045417786 | validation loss 0.0017334864907232778 | validation acc 0.6969068646430969\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNormSkip(vocab_size=len(voc), embedding_dim=64, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=20, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013559470377305532 | train acc 0.4994699954986572 | validation loss 0.001358712021955194 | validation acc 0.4976881146430969\n",
      "epoch 1 | train loss 0.0013543319594157138 | train acc 0.5016586780548096 | validation loss 0.0013544291456239488 | validation acc 0.5\n",
      "epoch 2 | train loss 0.0013543420784811878 | train acc 0.49842965602874756 | validation loss 0.0013544456855089841 | validation acc 0.4949178695678711\n",
      "epoch 3 | train loss 0.001354147722516396 | train acc 0.49920499324798584 | validation loss 0.0013575403771733828 | validation acc 0.4977877736091614\n",
      "epoch 4 | train loss 0.0013540660024704834 | train acc 0.501923680305481 | validation loss 0.0013556186084835125 | validation acc 0.4977877736091614\n",
      "epoch 5 | train loss 0.001353891017371742 | train acc 0.5047699213027954 | validation loss 0.0013567498826887459 | validation acc 0.496791273355484\n",
      "epoch 6 | train loss 0.001346038137597673 | train acc 0.5227406620979309 | validation loss 0.0012990312368076826 | validation acc 0.5622209906578064\n",
      "epoch 7 | train loss 0.0012799223423373871 | train acc 0.5726876258850098 | validation loss 0.0013482476791309916 | validation acc 0.5733218789100647\n",
      "epoch 8 | train loss 0.0012541988188279184 | train acc 0.5975580811500549 | validation loss 0.0013160378005764239 | validation acc 0.5829280614852905\n",
      "epoch 9 | train loss 0.0011767507448848618 | train acc 0.6448943614959717 | validation loss 0.0011998451034519442 | validation acc 0.6403858065605164\n",
      "epoch 10 | train loss 0.0011241174892091834 | train acc 0.6715511083602905 | validation loss 0.0011800720430055292 | validation acc 0.6602359414100647\n",
      "epoch 11 | train loss 0.0011032889146666656 | train acc 0.681601345539093 | validation loss 0.0011498371842412316 | validation acc 0.664461076259613\n",
      "epoch 12 | train loss 0.0010898641393117456 | train acc 0.6884618401527405 | validation loss 0.0011717868138494312 | validation acc 0.669921875\n",
      "epoch 13 | train loss 0.001078072974537897 | train acc 0.6951358318328857 | validation loss 0.0011382436181409095 | validation acc 0.6737483739852905\n",
      "epoch 14 | train loss 0.0010667035024760562 | train acc 0.6974717378616333 | validation loss 0.0011520943508188867 | validation acc 0.6753228306770325\n",
      "epoch 15 | train loss 0.0010562772653767624 | train acc 0.701132595539093 | validation loss 0.001140400745646971 | validation acc 0.6774154901504517\n",
      "epoch 16 | train loss 0.0010493787442247416 | train acc 0.7026048302650452 | validation loss 0.001146430679482921 | validation acc 0.6776546239852905\n",
      "epoch 17 | train loss 0.0010418401409207315 | train acc 0.7053627371788025 | validation loss 0.0011734945716797278 | validation acc 0.6805444955825806\n",
      "epoch 18 | train loss 0.0010330436169754846 | train acc 0.7067858576774597 | validation loss 0.001192373818449429 | validation acc 0.6780332922935486\n",
      "epoch 19 | train loss 0.0010254597458075826 | train acc 0.7099266052246094 | validation loss 0.001188729122300081 | validation acc 0.6818398833274841\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNormSkip(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=20, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013403827260159442 | train acc 0.5233491659164429 | validation loss 0.0017241551707099592 | validation acc 0.5920360088348389\n",
      "epoch 1 | train loss 0.001240032063691816 | train acc 0.6098755598068237 | validation loss 0.0013179327450379046 | validation acc 0.6538982391357422\n",
      "epoch 2 | train loss 0.001154696276345894 | train acc 0.6498115658760071 | validation loss 0.0013845679368015987 | validation acc 0.6711973547935486\n",
      "epoch 3 | train loss 0.0011365123651219747 | train acc 0.661363422870636 | validation loss 0.0014285328109007404 | validation acc 0.6622488498687744\n",
      "epoch 4 | train loss 0.0011268349524690352 | train acc 0.6668302416801453 | validation loss 0.0014791804524993866 | validation acc 0.6535594463348389\n",
      "epoch 5 | train loss 0.0011214934436875822 | train acc 0.6702260971069336 | validation loss 0.0014420482389419815 | validation acc 0.6436543464660645\n",
      "epoch 6 | train loss 0.0011124077538816942 | train acc 0.6734747886657715 | validation loss 0.0013403927453565507 | validation acc 0.6406648755073547\n",
      "epoch 7 | train loss 0.001105830031967764 | train acc 0.677763819694519 | validation loss 0.001358510233219523 | validation acc 0.6307796239852905\n",
      "epoch 8 | train loss 0.001102415095264766 | train acc 0.6801291704177856 | validation loss 0.0013538858703603701 | validation acc 0.6574856638908386\n",
      "epoch 9 | train loss 0.0010957667945144726 | train acc 0.6821706295013428 | validation loss 0.0013119049319208656 | validation acc 0.6407246589660645\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNorm(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=32, num_layers=1, dropout=0.7).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0013018478794449275 | train acc 0.5558554530143738 | validation loss 0.0011711976964891488 | validation acc 0.6596779227256775\n",
      "epoch 1 | train loss 0.0011402542776563884 | train acc 0.6577516198158264 | validation loss 0.001202167302241777 | validation acc 0.6741669178009033\n",
      "epoch 2 | train loss 0.0011178009787073649 | train acc 0.6743384599685669 | validation loss 0.001171552855877814 | validation acc 0.678730845451355\n",
      "epoch 3 | train loss 0.0011044401871730765 | train acc 0.6804922819137573 | validation loss 0.0011991791549490346 | validation acc 0.6815808415412903\n",
      "epoch 4 | train loss 0.0010892420104206377 | train acc 0.6859002113342285 | validation loss 0.0012132488791023058 | validation acc 0.6809829473495483\n",
      "epoch 5 | train loss 0.001079804403385634 | train acc 0.690100908279419 | validation loss 0.0011572947037258965 | validation acc 0.6826570630073547\n",
      "epoch 6 | train loss 0.0010678662573942101 | train acc 0.6939188241958618 | validation loss 0.0011581583637554124 | validation acc 0.6825574040412903\n",
      "epoch 7 | train loss 0.0010586859269041905 | train acc 0.6964019536972046 | validation loss 0.0011932589743305377 | validation acc 0.6796077489852905\n",
      "epoch 8 | train loss 0.0010493067122443612 | train acc 0.7014074325561523 | validation loss 0.0011983659772953133 | validation acc 0.649812638759613\n",
      "epoch 9 | train loss 0.0010366797373786887 | train acc 0.7070803046226501 | validation loss 0.001191108558227175 | validation acc 0.6714963316917419\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNorm(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0012933837604386048 | train acc 0.5534901022911072 | validation loss 0.0012164693569279828 | validation acc 0.6682676672935486\n",
      "epoch 1 | train loss 0.0011323114041585343 | train acc 0.6634637713432312 | validation loss 0.001199233786996492 | validation acc 0.6751633882522583\n",
      "epoch 2 | train loss 0.001117805551372506 | train acc 0.6739262938499451 | validation loss 0.0012308135218870807 | validation acc 0.6794881820678711\n",
      "epoch 3 | train loss 0.0011081920855388458 | train acc 0.6796090006828308 | validation loss 0.0012012850891143009 | validation acc 0.6790497303009033\n",
      "epoch 4 | train loss 0.001098237601787451 | train acc 0.6833778023719788 | validation loss 0.001261881645887672 | validation acc 0.6821189522743225\n",
      "epoch 5 | train loss 0.0010896623722539663 | train acc 0.6869994401931763 | validation loss 0.0012474327825713067 | validation acc 0.6837531924247742\n",
      "epoch 6 | train loss 0.0010814437363062747 | train acc 0.6915926933288574 | validation loss 0.0012321598615202749 | validation acc 0.6841318607330322\n",
      "epoch 7 | train loss 0.0010724918322752338 | train acc 0.6923975348472595 | validation loss 0.0012194506778875937 | validation acc 0.6832947731018066\n",
      "epoch 8 | train loss 0.0010623277518044146 | train acc 0.6974226236343384 | validation loss 0.0012301704056361423 | validation acc 0.6864835619926453\n",
      "epoch 9 | train loss 0.0010512613579654253 | train acc 0.7014760971069336 | validation loss 0.0012676968377045527 | validation acc 0.6856664419174194\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPoolingBatchNorm(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0012858616455514265 | train acc 0.5612731575965881 | validation loss 0.0012024736111479982 | validation acc 0.6549744606018066\n",
      "epoch 1 | train loss 0.0011384550197723897 | train acc 0.6562990546226501 | validation loss 0.0012086908658491258 | validation acc 0.6511678695678711\n",
      "epoch 2 | train loss 0.0011205152858834919 | train acc 0.671227216720581 | validation loss 0.0011971784121242864 | validation acc 0.664461076259613\n",
      "epoch 3 | train loss 0.0011107410986067718 | train acc 0.6770375370979309 | validation loss 0.0011827570765768653 | validation acc 0.6735491156578064\n",
      "epoch 4 | train loss 0.0011008755372949506 | train acc 0.6827104091644287 | validation loss 0.001205978445276352 | validation acc 0.6646803021430969\n",
      "epoch 5 | train loss 0.0010892117703707898 | train acc 0.688098669052124 | validation loss 0.0011959271282741648 | validation acc 0.6682676672935486\n",
      "epoch 6 | train loss 0.001080897159057777 | train acc 0.6904247999191284 | validation loss 0.001188324799414306 | validation acc 0.6636439561843872\n",
      "epoch 7 | train loss 0.0010691625659878193 | train acc 0.6958915591239929 | validation loss 0.0011965076294157427 | validation acc 0.6505898833274841\n",
      "epoch 8 | train loss 0.0010571146754480094 | train acc 0.6988948583602905 | validation loss 0.0012102709609150355 | validation acc 0.6744459271430969\n",
      "epoch 9 | train loss 0.0010467596475243212 | train acc 0.7032133340835571 | validation loss 0.001238996148161704 | validation acc 0.6407644748687744\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPooling(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=256, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.005183413676828633 | train acc 0.5648359060287476 | validation loss 0.0046833632724735015 | validation acc 0.6428372263908386\n",
      "epoch 1 | train loss 0.004651555127666645 | train acc 0.6439030766487122 | validation loss 0.00489229328689232 | validation acc 0.6694435477256775\n",
      "epoch 2 | train loss 0.0045464883649712386 | train acc 0.6621584296226501 | validation loss 0.005032389966428888 | validation acc 0.6725525856018066\n",
      "epoch 3 | train loss 0.004496879441296242 | train acc 0.6720222234725952 | validation loss 0.005196053567116757 | validation acc 0.6754025816917419\n",
      "epoch 4 | train loss 0.004458402633905673 | train acc 0.6767430901527405 | validation loss 0.005069197470628732 | validation acc 0.6788105964660645\n",
      "epoch 5 | train loss 0.004424692921477356 | train acc 0.6818369030952454 | validation loss 0.005014145906601215 | validation acc 0.6802056431770325\n",
      "epoch 6 | train loss 0.004391484078073678 | train acc 0.6862633228302002 | validation loss 0.004982398665208388 | validation acc 0.6820192933082581\n",
      "epoch 7 | train loss 0.00436947141506653 | train acc 0.6890997886657715 | validation loss 0.004991896203433981 | validation acc 0.6814014315605164\n",
      "epoch 8 | train loss 0.004337793841768505 | train acc 0.6929373145103455 | validation loss 0.004986240216103211 | validation acc 0.6818398833274841\n",
      "epoch 9 | train loss 0.004315979385791795 | train acc 0.6947628855705261 | validation loss 0.005085912477272581 | validation acc 0.6822783946990967\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPooling(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PICKED 2 (train 0.76 val 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.005084941087901387 | train acc 0.5771238803863525 | validation loss 0.004573788645627376 | validation acc 0.6524633169174194\n",
      "epoch 1 | train loss 0.004545036815960971 | train acc 0.6644158363342285 | validation loss 0.004549594325778473 | validation acc 0.6423788070678711\n",
      "epoch 2 | train loss 0.004422851363881421 | train acc 0.6838587522506714 | validation loss 0.004453614128430431 | validation acc 0.6788703799247742\n",
      "epoch 3 | train loss 0.004319671781741822 | train acc 0.6950376629829407 | validation loss 0.00441636916485197 | validation acc 0.6827965378761292\n",
      "epoch 4 | train loss 0.004250477465049799 | train acc 0.7027912735939026 | validation loss 0.0044119875840676415 | validation acc 0.6815210580825806\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPaddingBothWaysTransposed(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004185397648275947 | train acc 0.7107608318328857 | validation loss 0.004489231606403708 | validation acc 0.6811822652816772\n",
      "epoch 1 | train loss 0.004139450121607781 | train acc 0.7143334150314331 | validation loss 0.004438301369581106 | validation acc 0.68359375\n",
      "epoch 2 | train loss 0.004084396385032001 | train acc 0.7202614545822144 | validation loss 0.004566717815967942 | validation acc 0.6885363459587097\n",
      "epoch 3 | train loss 0.0040362114674673795 | train acc 0.7254239916801453 | validation loss 0.004452898099065797 | validation acc 0.6892139911651611\n",
      "epoch 4 | train loss 0.003989203225958147 | train acc 0.7280248999595642 | validation loss 0.004495862365655164 | validation acc 0.6903300285339355\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.003936590820617609 | train acc 0.7330107688903809 | validation loss 0.004581964254612103 | validation acc 0.6922632455825806\n",
      "epoch 1 | train loss 0.003888784655622539 | train acc 0.7381536364555359 | validation loss 0.004579380857259301 | validation acc 0.6929010152816772\n",
      "epoch 2 | train loss 0.003840530986035355 | train acc 0.7428058385848999 | validation loss 0.004635081197341847 | validation acc 0.6863042116165161\n",
      "epoch 3 | train loss 0.0037981283749470105 | train acc 0.7464765310287476 | validation loss 0.004614710099805071 | validation acc 0.6934789419174194\n",
      "epoch 4 | train loss 0.0037670996595564857 | train acc 0.7508244514465332 | validation loss 0.004577731046640333 | validation acc 0.6854870915412903\n",
      "epoch 5 | train loss 0.0037207996163250477 | train acc 0.754475474357605 | validation loss 0.004702297632394796 | validation acc 0.6855867505073547\n",
      "epoch 6 | train loss 0.0036821636743083054 | train acc 0.7591375112533569 | validation loss 0.004711351771684535 | validation acc 0.6825374960899353\n",
      "epoch 7 | train loss 0.003646127228366548 | train acc 0.7615911364555359 | validation loss 0.0048794682548685495 | validation acc 0.6949338316917419\n",
      "epoch 8 | train loss 0.003621337571311227 | train acc 0.7637209296226501 | validation loss 0.00498364520454019 | validation acc 0.6895926594734192\n",
      "epoch 9 | train loss 0.003590618924766629 | train acc 0.7660273909568787 | validation loss 0.0048414195358529874 | validation acc 0.6857262253761292\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004632107675983453 | train acc 0.6567112803459167 | validation loss 0.0044574606358857675 | validation acc 0.6833147406578064\n",
      "epoch 1 | train loss 0.004452804778585557 | train acc 0.6796678900718689 | validation loss 0.00444137543227229 | validation acc 0.6841318607330322\n",
      "epoch 2 | train loss 0.004441115043000856 | train acc 0.681002676486969 | validation loss 0.004445498577575675 | validation acc 0.6828563213348389\n",
      "epoch 3 | train loss 0.004436282550415315 | train acc 0.6821215748786926 | validation loss 0.004445788503995127 | validation acc 0.6831552982330322\n",
      "epoch 4 | train loss 0.004435591586723914 | train acc 0.6818271279335022 | validation loss 0.004444650366631508 | validation acc 0.6842514276504517\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPadding(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004629464534839063 | train acc 0.6592042446136475 | validation loss 0.004451352655970757 | validation acc 0.6830357313156128\n",
      "epoch 1 | train loss 0.004449036289427673 | train acc 0.6809830665588379 | validation loss 0.0044433740805121785 | validation acc 0.6823979616165161\n",
      "epoch 2 | train loss 0.004441115954433376 | train acc 0.6824159622192383 | validation loss 0.004444128343697676 | validation acc 0.6827168464660645\n",
      "epoch 3 | train loss 0.004436607050958637 | train acc 0.6819449067115784 | validation loss 0.004440417011358718 | validation acc 0.6838129758834839\n",
      "epoch 4 | train loss 0.004435394586844313 | train acc 0.6823570728302002 | validation loss 0.0044453041111261644 | validation acc 0.6810028553009033\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPadding(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.0046243099137455055 | train acc 0.6585564613342285 | validation loss 0.004455494706884825 | validation acc 0.6824178695678711\n",
      "epoch 1 | train loss 0.004453588319660385 | train acc 0.6789121627807617 | validation loss 0.004444048215506827 | validation acc 0.6846300959587097\n",
      "epoch 2 | train loss 0.0044435603499070895 | train acc 0.681002676486969 | validation loss 0.0044433617722646965 | validation acc 0.6837332844734192\n",
      "epoch 3 | train loss 0.004436718196585896 | train acc 0.6821706295013428 | validation loss 0.004442322553475197 | validation acc 0.683015763759613\n",
      "epoch 4 | train loss 0.004436661298304853 | train acc 0.6819350719451904 | validation loss 0.0044413141137682735 | validation acc 0.683195173740387\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTED(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004438100981873568 | train acc 0.6818663477897644 | validation loss 0.004450808284561891 | validation acc 0.6803252696990967\n",
      "epoch 1 | train loss 0.004435096486400223 | train acc 0.6823669075965881 | validation loss 0.00444878395694327 | validation acc 0.6819595098495483\n",
      "epoch 2 | train loss 0.004433769227126052 | train acc 0.6820822954177856 | validation loss 0.00444030267399099 | validation acc 0.6828961968421936\n",
      "epoch 3 | train loss 0.004434878244526899 | train acc 0.6826024651527405 | validation loss 0.00443860646830254 | validation acc 0.6841916441917419\n",
      "epoch 4 | train loss 0.004432957457968188 | train acc 0.682916522026062 | validation loss 0.004439978006507308 | validation acc 0.6829958558082581\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004606983171461207 | train acc 0.6592827439308167 | validation loss 0.004448186092100068 | validation acc 0.683573842048645\n",
      "epoch 1 | train loss 0.0044541016240644 | train acc 0.6798936128616333 | validation loss 0.004445147721396227 | validation acc 0.6831752061843872\n",
      "epoch 2 | train loss 0.00444555999746256 | train acc 0.6803450584411621 | validation loss 0.004443273654478431 | validation acc 0.6834343075752258\n",
      "epoch 3 | train loss 0.0044407088295778906 | train acc 0.681670069694519 | validation loss 0.004441302975019611 | validation acc 0.6839525103569031\n",
      "epoch 4 | train loss 0.004437167623347074 | train acc 0.680933952331543 | validation loss 0.004444432289848028 | validation acc 0.6830357313156128\n",
      "epoch 5 | train loss 0.0044356451003318515 | train acc 0.6824257969856262 | validation loss 0.00444732807554086 | validation acc 0.6842713356018066\n",
      "epoch 6 | train loss 0.004433529202717511 | train acc 0.6827889680862427 | validation loss 0.004444381952100452 | validation acc 0.6825574040412903\n",
      "epoch 7 | train loss 0.004434631171580123 | train acc 0.6827496886253357 | validation loss 0.004446503806357956 | validation acc 0.6838328838348389\n",
      "epoch 8 | train loss 0.004432729270628621 | train acc 0.6819645166397095 | validation loss 0.004441300080376392 | validation acc 0.6843311786651611\n",
      "epoch 9 | train loss 0.00443134965473346 | train acc 0.6828576326370239 | validation loss 0.004443669501319052 | validation acc 0.68359375\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004431699380400268 | train acc 0.6826319098472595 | validation loss 0.00444557509926737 | validation acc 0.6838927268981934\n",
      "epoch 1 | train loss 0.00443036327824165 | train acc 0.6833778023719788 | validation loss 0.004442813077452117 | validation acc 0.6842713356018066\n",
      "epoch 2 | train loss 0.004430697209011043 | train acc 0.6830637454986572 | validation loss 0.0044402719305457585 | validation acc 0.6842514276504517\n",
      "epoch 3 | train loss 0.00442907197199687 | train acc 0.6826809644699097 | validation loss 0.004440097894391273 | validation acc 0.6843510866165161\n",
      "epoch 4 | train loss 0.004429804837745357 | train acc 0.6826319098472595 | validation loss 0.004439055525557594 | validation acc 0.6830955147743225\n",
      "epoch 5 | train loss 0.004429921711415951 | train acc 0.6828674674034119 | validation loss 0.004439195501024132 | validation acc 0.683992326259613\n",
      "epoch 6 | train loss 0.0044280264489165505 | train acc 0.6831913590431213 | validation loss 0.004439981848509906 | validation acc 0.6837133169174194\n",
      "epoch 7 | train loss 0.004428182271556068 | train acc 0.6832305788993835 | validation loss 0.004439857825506193 | validation acc 0.6839126348495483\n",
      "epoch 8 | train loss 0.004428387976990984 | train acc 0.6833287477493286 | validation loss 0.004439728975127337 | validation acc 0.6832349896430969\n",
      "epoch 9 | train loss 0.004428071791806899 | train acc 0.6831422448158264 | validation loss 0.004439372268187034 | validation acc 0.6838927268981934\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004558818094226433 | train acc 0.667399525642395 | validation loss 0.004454303977711659 | validation acc 0.6842913031578064\n",
      "epoch 1 | train loss 0.004451525872912558 | train acc 0.6803745031356812 | validation loss 0.004450733713420792 | validation acc 0.6830556392669678\n",
      "epoch 2 | train loss 0.004445482194294975 | train acc 0.6813167333602905 | validation loss 0.004455863461623025 | validation acc 0.6834741830825806\n",
      "epoch 3 | train loss 0.0044437008830905894 | train acc 0.681002676486969 | validation loss 0.004449086251896711 | validation acc 0.6829360723495483\n",
      "epoch 4 | train loss 0.004439700977822962 | train acc 0.6817878484725952 | validation loss 0.004454112519647888 | validation acc 0.6798868179321289\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004562477396196222 | train acc 0.6657604575157166 | validation loss 0.004450621608435652 | validation acc 0.6825374960899353\n",
      "epoch 1 | train loss 0.004449456841825278 | train acc 0.6795501112937927 | validation loss 0.004446163879260801 | validation acc 0.6823979616165161\n",
      "epoch 2 | train loss 0.004444686722258415 | train acc 0.6808456182479858 | validation loss 0.004448350926098528 | validation acc 0.6832150816917419\n",
      "epoch 3 | train loss 0.004442044778466084 | train acc 0.6815326809883118 | validation loss 0.004442728167323738 | validation acc 0.6836734414100647\n",
      "epoch 4 | train loss 0.004437503202797274 | train acc 0.6813560128211975 | validation loss 0.004450901516009958 | validation acc 0.6836535334587097\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004566744389802208 | train acc 0.6652402877807617 | validation loss 0.004452310507577768 | validation acc 0.6779735088348389\n",
      "epoch 1 | train loss 0.004450675888085298 | train acc 0.6802665591239929 | validation loss 0.00445954530198146 | validation acc 0.6788703799247742\n",
      "epoch 2 | train loss 0.004445265197890094 | train acc 0.6811302900314331 | validation loss 0.004445237333278115 | validation acc 0.6811423897743225\n",
      "epoch 3 | train loss 0.004442772522379162 | train acc 0.6806100606918335 | validation loss 0.004444013541262616 | validation acc 0.68359375\n",
      "epoch 4 | train loss 0.0044400807542915935 | train acc 0.6816995143890381 | validation loss 0.004442508612184462 | validation acc 0.6838528513908386\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004562939379724613 | train acc 0.6657310128211975 | validation loss 0.004451121592939337 | validation acc 0.6804049611091614\n",
      "epoch 1 | train loss 0.004451848961993229 | train acc 0.6797659993171692 | validation loss 0.004445281202847861 | validation acc 0.6827567219734192\n",
      "epoch 2 | train loss 0.004443730473642392 | train acc 0.6796776652336121 | validation loss 0.004446334957812761 | validation acc 0.6839126348495483\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004561093941666701 | train acc 0.6672424674034119 | validation loss 0.004456883064727774 | validation acc 0.6819396018981934\n",
      "epoch 1 | train loss 0.004441026115442518 | train acc 0.6805904507637024 | validation loss 0.004460767029703842 | validation acc 0.6764987111091614\n",
      "epoch 2 | train loss 0.00443008779703134 | train acc 0.6815817356109619 | validation loss 0.004456883669968656 | validation acc 0.6793686151504517\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkipOnWordPositionTwoLayers(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1, dim_hidden_decoder=8).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004573507227434136 | train acc 0.6675958037376404 | validation loss 0.004451421496627035 | validation acc 0.682637095451355\n",
      "epoch 1 | train loss 0.004435483337439662 | train acc 0.6813756227493286 | validation loss 0.004449188586904154 | validation acc 0.6812220811843872\n",
      "epoch 2 | train loss 0.004429199820736554 | train acc 0.6815621256828308 | validation loss 0.004456665414701482 | validation acc 0.6779934763908386\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkipOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PICKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004600985258251391 | train acc 0.6613732576370239 | validation loss 0.004453893274850478 | validation acc 0.6813217401504517\n",
      "epoch 1 | train loss 0.004465438979683706 | train acc 0.6777932643890381 | validation loss 0.0044495356188996756 | validation acc 0.6800262928009033\n",
      "epoch 2 | train loss 0.004463747952363908 | train acc 0.6779306530952454 | validation loss 0.004447118593593679 | validation acc 0.6836734414100647\n",
      "epoch 3 | train loss 0.004458789922380071 | train acc 0.6799623370170593 | validation loss 0.004443505789005977 | validation acc 0.6834143996238708\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.004582824037046448 | train acc 0.66144198179245 | validation loss 0.004461520132893811 | validation acc 0.6788902878761292\n",
      "epoch 1 | train loss 0.004471411472818248 | train acc 0.675820529460907 | validation loss 0.0044550802915033945 | validation acc 0.6831353902816772\n",
      "epoch 2 | train loss 0.004463508199103616 | train acc 0.6763603091239929 | validation loss 0.0044491798919863166 | validation acc 0.6817801594734192\n",
      "epoch 3 | train loss 0.004463460496024242 | train acc 0.6776853203773499 | validation loss 0.004452569045813051 | validation acc 0.679109513759613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=32, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.005069418102748523 | train acc 0.6058515310287476 | validation loss 0.004597217420992746 | validation acc 0.6716158986091614\n",
      "epoch 1 | train loss 0.004528155480014235 | train acc 0.6707561016082764 | validation loss 0.004451946965988954 | validation acc 0.6833147406578064\n",
      "epoch 2 | train loss 0.004470993669635742 | train acc 0.6781269907951355 | validation loss 0.004442791472906627 | validation acc 0.683195173740387\n",
      "epoch 3 | train loss 0.0044613515777716965 | train acc 0.678342878818512 | validation loss 0.004443389930517641 | validation acc 0.6836535334587097\n",
      "epoch 4 | train loss 0.004452196304802198 | train acc 0.6785489916801453 | validation loss 0.00443882708394976 | validation acc 0.6840122938156128\n",
      "epoch 5 | train loss 0.004454163813831322 | train acc 0.6786667704582214 | validation loss 0.004440363247080573 | validation acc 0.6839525103569031\n",
      "epoch 6 | train loss 0.0044524092853172265 | train acc 0.6792851090431213 | validation loss 0.00443882212412249 | validation acc 0.6840919852256775\n",
      "epoch 7 | train loss 0.004450209004583362 | train acc 0.6793341636657715 | validation loss 0.0044375836456607915 | validation acc 0.6841517686843872\n",
      "epoch 8 | train loss 0.004447554669871776 | train acc 0.6786766052246094 | validation loss 0.004438789099293324 | validation acc 0.6834940910339355\n",
      "epoch 9 | train loss 0.00445050218136134 | train acc 0.6788139939308167 | validation loss 0.0044415536293835964 | validation acc 0.6840919852256775\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.005304634000698863 | train acc 0.5619994401931763 | validation loss 0.0050715585904462 | validation acc 0.6253387928009033\n",
      "epoch 1 | train loss 0.004846395652661634 | train acc 0.6425290703773499 | validation loss 0.004623212458385269 | validation acc 0.6627471446990967\n",
      "epoch 2 | train loss 0.004568052345924266 | train acc 0.6661824584007263 | validation loss 0.004484202303538783 | validation acc 0.6790696978569031\n",
      "epoch 3 | train loss 0.004485624396135731 | train acc 0.6772436499595642 | validation loss 0.004451954369350071 | validation acc 0.6827766299247742\n",
      "epoch 4 | train loss 0.0044629388908860295 | train acc 0.6784704923629761 | validation loss 0.0044476447451137465 | validation acc 0.6821588277816772\n",
      "epoch 5 | train loss 0.004455520204577277 | train acc 0.6794617772102356 | validation loss 0.004440744633475147 | validation acc 0.6832549571990967\n",
      "epoch 6 | train loss 0.004451952254412527 | train acc 0.6800702810287476 | validation loss 0.004441470330657095 | validation acc 0.6839724183082581\n",
      "epoch 7 | train loss 0.00444736263109839 | train acc 0.6790593862533569 | validation loss 0.004440521658338815 | validation acc 0.6834741830825806\n",
      "epoch 8 | train loss 0.0044470566223099375 | train acc 0.679648220539093 | validation loss 0.00443816316618147 | validation acc 0.683613657951355\n",
      "epoch 9 | train loss 0.004445987014859288 | train acc 0.6799819469451904 | validation loss 0.004438235604427565 | validation acc 0.6835339665412903\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.00005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.00509055516239134 | train acc 0.6039572954177856 | validation loss 0.004623973122334621 | validation acc 0.6701809763908386\n",
      "epoch 1 | train loss 0.00453963147662931 | train acc 0.6696078181266785 | validation loss 0.004458486681034294 | validation acc 0.6815210580825806\n",
      "epoch 2 | train loss 0.004476245543153151 | train acc 0.6769688129425049 | validation loss 0.004444234651674954 | validation acc 0.6826570630073547\n",
      "epoch 3 | train loss 0.004462162147676245 | train acc 0.6780288219451904 | validation loss 0.004446033453711366 | validation acc 0.6831752061843872\n",
      "epoch 4 | train loss 0.004456018370163192 | train acc 0.6785882711410522 | validation loss 0.004439338907475906 | validation acc 0.6836734414100647\n",
      "epoch 5 | train loss 0.004452455091821357 | train acc 0.6787158250808716 | validation loss 0.004451745376822406 | validation acc 0.6818199753761292\n",
      "epoch 6 | train loss 0.0044524114556732265 | train acc 0.6792163848876953 | validation loss 0.004440945487918465 | validation acc 0.6834940910339355\n",
      "epoch 7 | train loss 0.00445332216600836 | train acc 0.6785980463027954 | validation loss 0.00444087302056854 | validation acc 0.6828762888908386\n",
      "epoch 8 | train loss 0.004446850091583989 | train acc 0.6811498999595642 | validation loss 0.004440495406683743 | validation acc 0.6823780536651611\n",
      "epoch 9 | train loss 0.0044504644089769195 | train acc 0.6803450584411621 | validation loss 0.004441977486879166 | validation acc 0.682039201259613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.2).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
