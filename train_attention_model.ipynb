{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "display_name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dfc50ec6eac23905ec9d0de14b95fb04de7a27191c56780f2104afc692b71c20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 1: Attention TRAIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import help_functions\n",
    "import data_processor"
   ]
  },
  {
   "source": [
    "## STRUCTURE\n",
    "3. Build the model with embedding and Attention.\n",
    "5. Train on the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Build the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Get the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"saved_data/splitted_X_train_eq.pickle\")\n",
    "X_test = load_pickle_data(\"saved_data/splitted_X_test_eq.pickle\")\n",
    "y_train = load_pickle_data(\"saved_data/splitted_y_train_eq.pickle\")\n",
    "y_test = load_pickle_data(\"saved_data/splitted_y_test_eq.pickle\")\n",
    "voc = load_pickle_data(\"saved_data/all_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"attention_data/splitted_with_mask_X_train.pickle\")\n",
    "X_test = load_pickle_data(\"attention_data/splitted_with_mask_X_test.pickle\")\n",
    "y_train = load_pickle_data(\"attention_data/splitted_with_mask_y_train.pickle\")\n",
    "y_test = load_pickle_data(\"attention_data/splitted_with_mask_y_test.pickle\")\n",
    "mask_train = load_pickle_data(\"attention_data/splitted_with_mask_mask_train.pickle\")\n",
    "mask_test = load_pickle_data(\"attention_data/splitted_with_mask_mask_test.pickle\")\n",
    "voc = load_pickle_data(\"attention_data/splitted_with_mask_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training samples: 101905\nNumber of test samples: 50193\n\nSequence length per sample: 283\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {len(y_train)}')\n",
    "print(f'Number of test samples: {len(y_test)}')\n",
    "print(\"\")\n",
    "print(f'Sequence length per sample: {max_sequence_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, batch_size, mask=None):\n",
    "    random_indices = torch.randperm(len(x))\n",
    "    for i in range(0, len(x) - batch_size + 1, batch_size):\n",
    "        indices = random_indices[i:i+batch_size]\n",
    "        if not type(mask) == type(None):\n",
    "            yield x[indices].to(device), y[indices].to(device), mask[indices].to(device)\n",
    "        else:\n",
    "            yield x[indices].to(device), y[indices].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModel(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder = nn.Linear(embedding_dim*max_seq_len, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        # (batch_size, n_tokens, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        #print(f\"Reshaped output shape: {output.shape}\")\n",
    "        output = self.decoder(output)\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        #print(h)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithPooling(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithPooling, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pooler = nn.AvgPool1d(max_seq_len, stride=1)\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        # (batch_size, n_tokens, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        output = self.pooler(output.permute(0,2,1))\n",
    "        #print(f\"Pooled output shape: {output.shape}\")\n",
    "        output = self.decoder(output.view(-1,output.shape[1]))\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        #print(h)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithMaskOnWordPosition(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithMaskOnWordPosition, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, word_position_mask, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        #print(f\"Word position mask shape after unsqueeze: {word_position_mask.unsqueeze(-1).expand(src.shape).shape}\")\n",
    "        src = torch.masked_select(src, word_position_mask.unsqueeze(-1).expand(src.shape)).view(-1, 1, self.embedding_dim)\n",
    "        #print(f\"Masked output shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        output = self.decoder(output)\n",
    "        output = output.squeeze(2)\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "source": [
    "## Train the transformer!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by in batchify(X_train, y_train, batch_size):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(sample_weight)\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            accuracy = (output.eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by in batchify(X_test, y_test, batch_size):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mask(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by, bm in batchify(X_train, y_train, batch_size, mask_train):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx, bm)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(sample_weight)\n",
    "            #print(f\"by shape: {by.shape}\")\n",
    "            #print(f\"output shape: {output.shape}\")\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            accuracy = (output.eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by, bm in batchify(X_test, y_test, batch_size, mask_test):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx, bm)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_subset(sub_percentage, X_train, X_test, y_train, y_test):\n",
    "    train_sub_size = int(sub_percentage*len(y_train))\n",
    "    test_sub_size = int(sub_percentage*len(y_test))\n",
    "\n",
    "    X_train_sub = X_train[:train_sub_size]\n",
    "    X_test_sub = X_test[:test_sub_size]\n",
    "    y_train_sub = y_train[:train_sub_size]\n",
    "    y_test_sub = y_test[:test_sub_size]\n",
    "\n",
    "    return X_train_sub, X_test_sub, y_train_sub, y_test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.7985761088709677 | train acc 0.48891130089759827 | validation loss 0.771484375 | validation acc 0.5062500238418579\n",
      "epoch 1 | train loss 0.8001512096774194 | train acc 0.4879032373428345 | validation loss 0.7584635416666666 | validation acc 0.5145833492279053\n",
      "epoch 2 | train loss 0.8033014112903226 | train acc 0.4858871102333069 | validation loss 0.751953125 | validation acc 0.518750011920929\n",
      "epoch 3 | train loss 0.8048765120967742 | train acc 0.4848790466785431 | validation loss 0.7649739583333334 | validation acc 0.5104166865348816\n",
      "epoch 4 | train loss 0.8001512096774194 | train acc 0.4879032373428345 | validation loss 0.771484375 | validation acc 0.5062500238418579\n"
     ]
    }
   ],
   "source": [
    "data_subset = get_data_subset(0.01, X_train, X_test, y_train, y_test)\n",
    "\n",
    "model = MyAttentionModel(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train(model, torch.LongTensor(data_subset[0]), torch.LongTensor(data_subset[1]), torch.LongTensor(data_subset[2]), torch.LongTensor(data_subset[3]), n_epochs=5, batch_size=32, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.3897662138819096 | train acc 0.5010992288589478 | validation loss 0.3924155721859056 | validation acc 0.4977080821990967\n",
      "epoch 1 | train loss 0.3897738816151068 | train acc 0.5010894536972046 | validation loss 0.39236886160714285 | validation acc 0.4977678656578064\n",
      "epoch 2 | train loss 0.3897432106823178 | train acc 0.5011286735534668 | validation loss 0.3923532914142219 | validation acc 0.4977877736091614\n",
      "epoch 3 | train loss 0.3897662138819096 | train acc 0.5010992288589478 | validation loss 0.39238443180006377 | validation acc 0.49774792790412903\n",
      "epoch 4 | train loss 0.3897585461487123 | train acc 0.5011090636253357 | validation loss 0.39243114237882654 | validation acc 0.4976881444454193\n"
     ]
    }
   ],
   "source": [
    "data_subset = get_data_subset(1, X_train, X_test, y_train, y_test)\n",
    "\n",
    "model = MyAttentionModelWithPooling(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train(model, torch.LongTensor(data_subset[0]), torch.LongTensor(data_subset[1]), torch.LongTensor(data_subset[2]), torch.LongTensor(data_subset[3]), n_epochs=5, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.3897662138819096 | train acc 0.5010992288589478 | validation loss 0.39236886160714285 | validation acc 0.4977678656578064\n",
      "epoch 1 | train loss 0.3897738816151068 | train acc 0.5010894536972046 | validation loss 0.39236886160714285 | validation acc 0.4977678656578064\n",
      "epoch 2 | train loss 0.3897662138819096 | train acc 0.5010992288589478 | validation loss 0.39243114237882654 | validation acc 0.4976881444454193\n"
     ]
    }
   ],
   "source": [
    "#data_subset = get_data_subset(1, X_train, X_test, y_train, y_test)\n",
    "\n",
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyAttentionModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=2, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(8420, 16)\n",
       "  (decoder): Linear(in_features=4528, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8650"
      ]
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "nbr_params = 0\n",
    "for parameter in model.parameters():\n",
    "    nbr_params = nbr_params + len(parameter)\n",
    "nbr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyAttentionModelWithPooling(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}