{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "display_name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dfc50ec6eac23905ec9d0de14b95fb04de7a27191c56780f2104afc692b71c20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 1: Attention TRAIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import help_functions\n",
    "import data_processor\n",
    "import attention_models\n",
    "import better_attention_models"
   ]
  },
  {
   "source": [
    "## STRUCTURE\n",
    "3. Build the model with embedding and Attention.\n",
    "5. Train on the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Build the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Get the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"saved_data/splitted_X_train_eq.pickle\")\n",
    "X_test = load_pickle_data(\"saved_data/splitted_X_test_eq.pickle\")\n",
    "y_train = load_pickle_data(\"saved_data/splitted_y_train_eq.pickle\")\n",
    "y_test = load_pickle_data(\"saved_data/splitted_y_test_eq.pickle\")\n",
    "voc = load_pickle_data(\"saved_data/all_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"attention_data/splitted_with_mask_X_train.pickle\")\n",
    "X_test = load_pickle_data(\"attention_data/splitted_with_mask_X_test.pickle\")\n",
    "y_train = load_pickle_data(\"attention_data/splitted_with_mask_y_train.pickle\")\n",
    "y_test = load_pickle_data(\"attention_data/splitted_with_mask_y_test.pickle\")\n",
    "mask_train = load_pickle_data(\"attention_data/splitted_with_mask_mask_train.pickle\")\n",
    "mask_test = load_pickle_data(\"attention_data/splitted_with_mask_mask_test.pickle\")\n",
    "voc = load_pickle_data(\"attention_data/splitted_with_mask_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training samples: 101905\nNumber of test samples: 50193\n\nSequence length per sample: 283\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {len(y_train)}')\n",
    "print(f'Number of test samples: {len(y_test)}')\n",
    "print(\"\")\n",
    "print(f'Sequence length per sample: {max_sequence_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, batch_size, mask=None):\n",
    "    random_indices = torch.randperm(len(x))\n",
    "    for i in range(0, len(x) - batch_size + 1, batch_size):\n",
    "        indices = random_indices[i:i+batch_size]\n",
    "        if not type(mask) == type(None):\n",
    "            yield x[indices].to(device), y[indices].to(device), mask[indices].to(device)\n",
    "        else:\n",
    "            yield x[indices].to(device), y[indices].to(device)"
   ]
  },
  {
   "source": [
    "## Train the transformer!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by in batchify(X_train, y_train, batch_size):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(sample_weight)\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            accuracy = (output.eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by in batchify(X_test, y_test, batch_size):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mask(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5, padding_index=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by, bm in batchify(X_train, y_train, batch_size, mask_train):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            if padding_index is not None:\n",
    "                #print(\"using padding\")\n",
    "                padding_mask = (bx == padding_index)\n",
    "                #print(f\"padding_mask shape: {padding_mask.shape}\")\n",
    "                #print(f\"bx shape: {bx.shape}\")\n",
    "                #print(f\"padding_mask: {padding_mask}\")\n",
    "                output = model.forward(bx, bm, has_mask=True, padding_mask=padding_mask)\n",
    "            else:\n",
    "                #print(\"not using padding\")\n",
    "                output = model.forward(bx, bm)\n",
    "\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(\"Output after word position mask selection: \")\n",
    "            #print(torch.masked_select(bx, bm))\n",
    "            #print(sample_weight)\n",
    "            #print(f\"by shape: {by.shape}\")\n",
    "            #print(f\"output shape: {output.shape}\")\n",
    "            #print(f\"output: {output}\")\n",
    "            #print(f\"bx: {bx}\")\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            #print(f\"output rounded: {output.round()}\")\n",
    "            #print(f\"by: {by}\")\n",
    "            accuracy = (output.round().eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by, bm in batchify(X_test, y_test, batch_size, mask_test):\n",
    "                nbr_test_batches += 1\n",
    "                if padding_index is not None:\n",
    "                    padding_mask = (bx == padding_index)\n",
    "                    output = model.forward(bx, bm, has_mask=True, padding_mask=padding_mask)\n",
    "                else:\n",
    "                    output = model.forward(bx, bm)\n",
    "\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.round().eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_subset(sub_percentage, X_train, X_test, y_train, y_test):\n",
    "    train_sub_size = int(sub_percentage*len(y_train))\n",
    "    test_sub_size = int(sub_percentage*len(y_test))\n",
    "\n",
    "    X_train_sub = X_train[:train_sub_size]\n",
    "    X_test_sub = X_test[:test_sub_size]\n",
    "    y_train_sub = y_train[:train_sub_size]\n",
    "    y_test_sub = y_test[:test_sub_size]\n",
    "\n",
    "    return X_train_sub, X_test_sub, y_train_sub, y_test_sub"
   ]
  },
  {
   "source": [
    "## HANDLE MASK OF INPUT? (pad_token)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'attention_models' from '/Users/lovhag/Projects/dl4nlp_assignment_1/attention_models.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(attention_models)\n",
    "importlib.reload(better_attention_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004919881224439213 | train acc 0.6069507598876953 | validation loss 0.004498981557102228 | validation acc 0.680086076259613\n",
      "epoch 1 | train loss 0.004431075947979306 | train acc 0.6808358430862427 | validation loss 0.004439097912019898 | validation acc 0.6851283311843872\n",
      "epoch 2 | train loss 0.004348251864199419 | train acc 0.6897966265678406 | validation loss 0.004372248389254495 | validation acc 0.6886559128761292\n",
      "epoch 3 | train loss 0.00427969527298037 | train acc 0.6963430643081665 | validation loss 0.004361356068069914 | validation acc 0.6876395344734192\n",
      "epoch 4 | train loss 0.004213247361333208 | train acc 0.7022809386253357 | validation loss 0.004348757522054758 | validation acc 0.6879384517669678\n",
      "epoch 5 | train loss 0.0041504913783660296 | train acc 0.7061970233917236 | validation loss 0.004410814515276983 | validation acc 0.6881775856018066\n",
      "epoch 6 | train loss 0.004082272804006419 | train acc 0.7113202810287476 | validation loss 0.004372191947423948 | validation acc 0.683573842048645\n",
      "epoch 7 | train loss 0.004027385951719486 | train acc 0.7150204181671143 | validation loss 0.004412942775049988 | validation acc 0.6849888563156128\n",
      "epoch 8 | train loss 0.0039823271097830645 | train acc 0.7186616659164429 | validation loss 0.004469949409379909 | validation acc 0.6627471446990967\n",
      "epoch 9 | train loss 0.003932963721231843 | train acc 0.7208405137062073 | validation loss 0.004630819386838251 | validation acc 0.6432756781578064\n",
      "epoch 10 | train loss 0.0038836006079238827 | train acc 0.7246878743171692 | validation loss 0.004676508939081366 | validation acc 0.6396085619926453\n",
      "epoch 11 | train loss 0.0038378216746267884 | train acc 0.7293400764465332 | validation loss 0.004703887581483138 | validation acc 0.6575255393981934\n",
      "epoch 12 | train loss 0.003795500106444985 | train acc 0.7339137196540833 | validation loss 0.0049770283714716075 | validation acc 0.650410532951355\n",
      "epoch 13 | train loss 0.003751917152072364 | train acc 0.7366421818733215 | validation loss 0.005080034609465879 | validation acc 0.6409637928009033\n",
      "epoch 14 | train loss 0.0037014666750521666 | train acc 0.7437676787376404 | validation loss 0.0050107121484815045 | validation acc 0.6588408946990967\n",
      "epoch 15 | train loss 0.003664342903569528 | train acc 0.7474678158760071 | validation loss 0.005318974333931692 | validation acc 0.6516860723495483\n",
      "epoch 16 | train loss 0.00362265019179743 | train acc 0.750029444694519 | validation loss 0.005524227433666891 | validation acc 0.6434749960899353\n",
      "epoch 17 | train loss 0.003584713938771209 | train acc 0.7548582553863525 | validation loss 0.005483488493767681 | validation acc 0.6471420526504517\n",
      "epoch 18 | train loss 0.0035474768836266505 | train acc 0.7592552900314331 | validation loss 0.005449204681600368 | validation acc 0.6561703085899353\n",
      "epoch 19 | train loss 0.00350856095032963 | train acc 0.763711154460907 | validation loss 0.005856574512127198 | validation acc 0.6401466727256775\n"
     ]
    }
   ],
   "source": [
    "model = better_attention_models.MyAttentionModelWithPooling(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.2).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=20, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "source": [
    "# PICKED 2 (train 0.76 val 0.68)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005084941087901387 | train acc 0.5771238803863525 | validation loss 0.004573788645627376 | validation acc 0.6524633169174194\n",
      "epoch 1 | train loss 0.004545036815960971 | train acc 0.6644158363342285 | validation loss 0.004549594325778473 | validation acc 0.6423788070678711\n",
      "epoch 2 | train loss 0.004422851363881421 | train acc 0.6838587522506714 | validation loss 0.004453614128430431 | validation acc 0.6788703799247742\n",
      "epoch 3 | train loss 0.004319671781741822 | train acc 0.6950376629829407 | validation loss 0.00441636916485197 | validation acc 0.6827965378761292\n",
      "epoch 4 | train loss 0.004250477465049799 | train acc 0.7027912735939026 | validation loss 0.0044119875840676415 | validation acc 0.6815210580825806\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPaddingBothWaysTransposed(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004185397648275947 | train acc 0.7107608318328857 | validation loss 0.004489231606403708 | validation acc 0.6811822652816772\n",
      "epoch 1 | train loss 0.004139450121607781 | train acc 0.7143334150314331 | validation loss 0.004438301369581106 | validation acc 0.68359375\n",
      "epoch 2 | train loss 0.004084396385032001 | train acc 0.7202614545822144 | validation loss 0.004566717815967942 | validation acc 0.6885363459587097\n",
      "epoch 3 | train loss 0.0040362114674673795 | train acc 0.7254239916801453 | validation loss 0.004452898099065797 | validation acc 0.6892139911651611\n",
      "epoch 4 | train loss 0.003989203225958147 | train acc 0.7280248999595642 | validation loss 0.004495862365655164 | validation acc 0.6903300285339355\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.003936590820617609 | train acc 0.7330107688903809 | validation loss 0.004581964254612103 | validation acc 0.6922632455825806\n",
      "epoch 1 | train loss 0.003888784655622539 | train acc 0.7381536364555359 | validation loss 0.004579380857259301 | validation acc 0.6929010152816772\n",
      "epoch 2 | train loss 0.003840530986035355 | train acc 0.7428058385848999 | validation loss 0.004635081197341847 | validation acc 0.6863042116165161\n",
      "epoch 3 | train loss 0.0037981283749470105 | train acc 0.7464765310287476 | validation loss 0.004614710099805071 | validation acc 0.6934789419174194\n",
      "epoch 4 | train loss 0.0037670996595564857 | train acc 0.7508244514465332 | validation loss 0.004577731046640333 | validation acc 0.6854870915412903\n",
      "epoch 5 | train loss 0.0037207996163250477 | train acc 0.754475474357605 | validation loss 0.004702297632394796 | validation acc 0.6855867505073547\n",
      "epoch 6 | train loss 0.0036821636743083054 | train acc 0.7591375112533569 | validation loss 0.004711351771684535 | validation acc 0.6825374960899353\n",
      "epoch 7 | train loss 0.003646127228366548 | train acc 0.7615911364555359 | validation loss 0.0048794682548685495 | validation acc 0.6949338316917419\n",
      "epoch 8 | train loss 0.003621337571311227 | train acc 0.7637209296226501 | validation loss 0.00498364520454019 | validation acc 0.6895926594734192\n",
      "epoch 9 | train loss 0.003590618924766629 | train acc 0.7660273909568787 | validation loss 0.0048414195358529874 | validation acc 0.6857262253761292\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005421715691432956 | train acc 0.5005103349685669 | validation loss 0.005415573998292604 | validation acc 0.49705037474632263\n",
      "epoch 1 | train loss 0.00541595330206799 | train acc 0.49923446774482727 | validation loss 0.005415838692878962 | validation acc 0.49968111515045166\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-fe56060c066b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPaddingBothWaysTransposed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"___PAD___\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-93778f1d1df1>\u001b[0m in \u001b[0;36mtrain_with_mask\u001b[0;34m(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs, batch_size, lr, max_samples, weight_true, padding_index)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpadding_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dl4nlp_assignment_1/attention_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, word_position_mask, has_mask, padding_mask)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m#print(f\"Positional encoding shape: {src.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m#print(f\"src mask shape: {self.src_mask.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0;31m#print(f\"Transformer encoder output shape: {output.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_position_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[1;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4136\u001b[0m         \u001b[0;31m# average attention weights over heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPaddingBothWaysTransposed(vocab_size=len(voc), embedding_dim=4, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=4, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004632107675983453 | train acc 0.6567112803459167 | validation loss 0.0044574606358857675 | validation acc 0.6833147406578064\n",
      "epoch 1 | train loss 0.004452804778585557 | train acc 0.6796678900718689 | validation loss 0.00444137543227229 | validation acc 0.6841318607330322\n",
      "epoch 2 | train loss 0.004441115043000856 | train acc 0.681002676486969 | validation loss 0.004445498577575675 | validation acc 0.6828563213348389\n",
      "epoch 3 | train loss 0.004436282550415315 | train acc 0.6821215748786926 | validation loss 0.004445788503995127 | validation acc 0.6831552982330322\n",
      "epoch 4 | train loss 0.004435591586723914 | train acc 0.6818271279335022 | validation loss 0.004444650366631508 | validation acc 0.6842514276504517\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPadding(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004629464534839063 | train acc 0.6592042446136475 | validation loss 0.004451352655970757 | validation acc 0.6830357313156128\n",
      "epoch 1 | train loss 0.004449036289427673 | train acc 0.6809830665588379 | validation loss 0.0044433740805121785 | validation acc 0.6823979616165161\n",
      "epoch 2 | train loss 0.004441115954433376 | train acc 0.6824159622192383 | validation loss 0.004444128343697676 | validation acc 0.6827168464660645\n",
      "epoch 3 | train loss 0.004436607050958637 | train acc 0.6819449067115784 | validation loss 0.004440417011358718 | validation acc 0.6838129758834839\n",
      "epoch 4 | train loss 0.004435394586844313 | train acc 0.6823570728302002 | validation loss 0.0044453041111261644 | validation acc 0.6810028553009033\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTEDwithMaskOnPadding(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.0046243099137455055 | train acc 0.6585564613342285 | validation loss 0.004455494706884825 | validation acc 0.6824178695678711\n",
      "epoch 1 | train loss 0.004453588319660385 | train acc 0.6789121627807617 | validation loss 0.004444048215506827 | validation acc 0.6846300959587097\n",
      "epoch 2 | train loss 0.0044435603499070895 | train acc 0.681002676486969 | validation loss 0.0044433617722646965 | validation acc 0.6837332844734192\n",
      "epoch 3 | train loss 0.004436718196585896 | train acc 0.6821706295013428 | validation loss 0.004442322553475197 | validation acc 0.683015763759613\n",
      "epoch 4 | train loss 0.004436661298304853 | train acc 0.6819350719451904 | validation loss 0.0044413141137682735 | validation acc 0.683195173740387\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipCORRECTED(vocab_size=len(voc), embedding_dim=8, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=8, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004438100981873568 | train acc 0.6818663477897644 | validation loss 0.004450808284561891 | validation acc 0.6803252696990967\n",
      "epoch 1 | train loss 0.004435096486400223 | train acc 0.6823669075965881 | validation loss 0.00444878395694327 | validation acc 0.6819595098495483\n",
      "epoch 2 | train loss 0.004433769227126052 | train acc 0.6820822954177856 | validation loss 0.00444030267399099 | validation acc 0.6828961968421936\n",
      "epoch 3 | train loss 0.004434878244526899 | train acc 0.6826024651527405 | validation loss 0.00443860646830254 | validation acc 0.6841916441917419\n",
      "epoch 4 | train loss 0.004432957457968188 | train acc 0.682916522026062 | validation loss 0.004439978006507308 | validation acc 0.6829958558082581\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using padding\npadding_mask shape: torch.Size([64, 283])\nbx shape: torch.Size([64, 283])\nPositional encoding shape: torch.Size([64, 283, 16])\nPadding mask shape: torch.Size([64, 283])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-0bacb05e1b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipPlusMaskOnPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"___PAD___\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-f39f62b5eb32>\u001b[0m in \u001b[0;36mtrain_with_mask\u001b[0;34m(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs, batch_size, lr, max_samples, weight_true, padding_index)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"padding_mask shape: {padding_mask.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"bx shape: {bx.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not using padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/dl4nlp_assignment_1/attention_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, word_position_mask, padding_mask)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Padding mask shape: {padding_mask.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;31m#print(f\"Transformer encoder output shape: {output.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_position_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m#print(f\"Masked output shape: {output.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[1;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4096\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4097\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkipPlusMaskOnPadding(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5, padding_index=voc.stoi[\"___PAD___\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004606983171461207 | train acc 0.6592827439308167 | validation loss 0.004448186092100068 | validation acc 0.683573842048645\n",
      "epoch 1 | train loss 0.0044541016240644 | train acc 0.6798936128616333 | validation loss 0.004445147721396227 | validation acc 0.6831752061843872\n",
      "epoch 2 | train loss 0.00444555999746256 | train acc 0.6803450584411621 | validation loss 0.004443273654478431 | validation acc 0.6834343075752258\n",
      "epoch 3 | train loss 0.0044407088295778906 | train acc 0.681670069694519 | validation loss 0.004441302975019611 | validation acc 0.6839525103569031\n",
      "epoch 4 | train loss 0.004437167623347074 | train acc 0.680933952331543 | validation loss 0.004444432289848028 | validation acc 0.6830357313156128\n",
      "epoch 5 | train loss 0.0044356451003318515 | train acc 0.6824257969856262 | validation loss 0.00444732807554086 | validation acc 0.6842713356018066\n",
      "epoch 6 | train loss 0.004433529202717511 | train acc 0.6827889680862427 | validation loss 0.004444381952100452 | validation acc 0.6825574040412903\n",
      "epoch 7 | train loss 0.004434631171580123 | train acc 0.6827496886253357 | validation loss 0.004446503806357956 | validation acc 0.6838328838348389\n",
      "epoch 8 | train loss 0.004432729270628621 | train acc 0.6819645166397095 | validation loss 0.004441300080376392 | validation acc 0.6843311786651611\n",
      "epoch 9 | train loss 0.00443134965473346 | train acc 0.6828576326370239 | validation loss 0.004443669501319052 | validation acc 0.68359375\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.5).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004431699380400268 | train acc 0.6826319098472595 | validation loss 0.00444557509926737 | validation acc 0.6838927268981934\n",
      "epoch 1 | train loss 0.00443036327824165 | train acc 0.6833778023719788 | validation loss 0.004442813077452117 | validation acc 0.6842713356018066\n",
      "epoch 2 | train loss 0.004430697209011043 | train acc 0.6830637454986572 | validation loss 0.0044402719305457585 | validation acc 0.6842514276504517\n",
      "epoch 3 | train loss 0.00442907197199687 | train acc 0.6826809644699097 | validation loss 0.004440097894391273 | validation acc 0.6843510866165161\n",
      "epoch 4 | train loss 0.004429804837745357 | train acc 0.6826319098472595 | validation loss 0.004439055525557594 | validation acc 0.6830955147743225\n",
      "epoch 5 | train loss 0.004429921711415951 | train acc 0.6828674674034119 | validation loss 0.004439195501024132 | validation acc 0.683992326259613\n",
      "epoch 6 | train loss 0.0044280264489165505 | train acc 0.6831913590431213 | validation loss 0.004439981848509906 | validation acc 0.6837133169174194\n",
      "epoch 7 | train loss 0.004428182271556068 | train acc 0.6832305788993835 | validation loss 0.004439857825506193 | validation acc 0.6839126348495483\n",
      "epoch 8 | train loss 0.004428387976990984 | train acc 0.6833287477493286 | validation loss 0.004439728975127337 | validation acc 0.6832349896430969\n",
      "epoch 9 | train loss 0.004428071791806899 | train acc 0.6831422448158264 | validation loss 0.004439372268187034 | validation acc 0.6838927268981934\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_mask(trained_model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004558818094226433 | train acc 0.667399525642395 | validation loss 0.004454303977711659 | validation acc 0.6842913031578064\n",
      "epoch 1 | train loss 0.004451525872912558 | train acc 0.6803745031356812 | validation loss 0.004450733713420792 | validation acc 0.6830556392669678\n",
      "epoch 2 | train loss 0.004445482194294975 | train acc 0.6813167333602905 | validation loss 0.004455863461623025 | validation acc 0.6834741830825806\n",
      "epoch 3 | train loss 0.0044437008830905894 | train acc 0.681002676486969 | validation loss 0.004449086251896711 | validation acc 0.6829360723495483\n",
      "epoch 4 | train loss 0.004439700977822962 | train acc 0.6817878484725952 | validation loss 0.004454112519647888 | validation acc 0.6798868179321289\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004562477396196222 | train acc 0.6657604575157166 | validation loss 0.004450621608435652 | validation acc 0.6825374960899353\n",
      "epoch 1 | train loss 0.004449456841825278 | train acc 0.6795501112937927 | validation loss 0.004446163879260801 | validation acc 0.6823979616165161\n",
      "epoch 2 | train loss 0.004444686722258415 | train acc 0.6808456182479858 | validation loss 0.004448350926098528 | validation acc 0.6832150816917419\n",
      "epoch 3 | train loss 0.004442044778466084 | train acc 0.6815326809883118 | validation loss 0.004442728167323738 | validation acc 0.6836734414100647\n",
      "epoch 4 | train loss 0.004437503202797274 | train acc 0.6813560128211975 | validation loss 0.004450901516009958 | validation acc 0.6836535334587097\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004566744389802208 | train acc 0.6652402877807617 | validation loss 0.004452310507577768 | validation acc 0.6779735088348389\n",
      "epoch 1 | train loss 0.004450675888085298 | train acc 0.6802665591239929 | validation loss 0.00445954530198146 | validation acc 0.6788703799247742\n",
      "epoch 2 | train loss 0.004445265197890094 | train acc 0.6811302900314331 | validation loss 0.004445237333278115 | validation acc 0.6811423897743225\n",
      "epoch 3 | train loss 0.004442772522379162 | train acc 0.6806100606918335 | validation loss 0.004444013541262616 | validation acc 0.68359375\n",
      "epoch 4 | train loss 0.0044400807542915935 | train acc 0.6816995143890381 | validation loss 0.004442508612184462 | validation acc 0.6838528513908386\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004562939379724613 | train acc 0.6657310128211975 | validation loss 0.004451121592939337 | validation acc 0.6804049611091614\n",
      "epoch 1 | train loss 0.004451848961993229 | train acc 0.6797659993171692 | validation loss 0.004445281202847861 | validation acc 0.6827567219734192\n",
      "epoch 2 | train loss 0.004443730473642392 | train acc 0.6796776652336121 | validation loss 0.004446334957812761 | validation acc 0.6839126348495483\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndMaskedSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004561093941666701 | train acc 0.6672424674034119 | validation loss 0.004456883064727774 | validation acc 0.6819396018981934\n",
      "epoch 1 | train loss 0.004441026115442518 | train acc 0.6805904507637024 | validation loss 0.004460767029703842 | validation acc 0.6764987111091614\n",
      "epoch 2 | train loss 0.00443008779703134 | train acc 0.6815817356109619 | validation loss 0.004456883669968656 | validation acc 0.6793686151504517\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkipOnWordPositionTwoLayers(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1, dim_hidden_decoder=8).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004573507227434136 | train acc 0.6675958037376404 | validation loss 0.004451421496627035 | validation acc 0.682637095451355\n",
      "epoch 1 | train loss 0.004435483337439662 | train acc 0.6813756227493286 | validation loss 0.004449188586904154 | validation acc 0.6812220811843872\n",
      "epoch 2 | train loss 0.004429199820736554 | train acc 0.6815621256828308 | validation loss 0.004456665414701482 | validation acc 0.6779934763908386\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkipOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004545213624948505 | train acc 0.6680472493171692 | validation loss 0.004450402385020647 | validation acc 0.6837731003761292\n",
      "epoch 1 | train loss 0.004441837943246794 | train acc 0.6808063983917236 | validation loss 0.004455881574005839 | validation acc 0.6787109375\n",
      "epoch 2 | train loss 0.004430041527205928 | train acc 0.6810517311096191 | validation loss 0.004459751671006694 | validation acc 0.6799466013908386\n",
      "epoch 3 | train loss 0.0044234250729199235 | train acc 0.6824552416801453 | validation loss 0.004465781026626747 | validation acc 0.680644154548645\n",
      "epoch 4 | train loss 0.004414622861588954 | train acc 0.6838980317115784 | validation loss 0.004474507580031355 | validation acc 0.6763990521430969\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-18c8bbd5ecb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMyAttentionModelWithPoolingAndSkipOnWordPosition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c4df01346df4>\u001b[0m in \u001b[0;36mtrain_with_mask\u001b[0;34m(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs, batch_size, lr, max_samples, weight_true)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_samples\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkipOnWordPosition(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005416679883729105 | train acc 0.505790650844574 | validation loss 0.005410850713295596 | validation acc 0.522062361240387\n",
      "epoch 1 | train loss 0.00539411854551141 | train acc 0.5345379114151001 | validation loss 0.0053290227524299474 | validation acc 0.6283681392669678\n",
      "epoch 2 | train loss 0.005162442772562648 | train acc 0.6292497515678406 | validation loss 0.004978231641425922 | validation acc 0.652742326259613\n",
      "epoch 3 | train loss 0.00485021103057664 | train acc 0.6664474606513977 | validation loss 0.004828228293894315 | validation acc 0.6515864133834839\n",
      "epoch 4 | train loss 0.0046802696511009005 | train acc 0.6747801303863525 | validation loss 0.004775377461741374 | validation acc 0.625996470451355\n",
      "epoch 5 | train loss 0.004580687657940487 | train acc 0.6788238286972046 | validation loss 0.004776985090218807 | validation acc 0.6235650777816772\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-21c8387960f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMyAttentionModelWithPoolingAndSkip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c4df01346df4>\u001b[0m in \u001b[0;36mtrain_with_mask\u001b[0;34m(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs, batch_size, lr, max_samples, weight_true)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_samples\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/dl4nlp_assignment_1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithPoolingAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.0045634866499075265 | train acc 0.6658095121383667 | validation loss 0.0044626091501190875 | validation acc 0.6758809089660645\n",
      "epoch 1 | train loss 0.004447784168053718 | train acc 0.6799230575561523 | validation loss 0.004450265819156941 | validation acc 0.6823381781578064\n",
      "epoch 2 | train loss 0.00443883963358055 | train acc 0.6803745031356812 | validation loss 0.00445738075329561 | validation acc 0.6811822652816772\n"
     ]
    }
   ],
   "source": [
    "model = attention_models.MyAttentionModelWithMaskOnWordPositionOutputAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=3, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "# PICKED"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004600985258251391 | train acc 0.6613732576370239 | validation loss 0.004453893274850478 | validation acc 0.6813217401504517\n",
      "epoch 1 | train loss 0.004465438979683706 | train acc 0.6777932643890381 | validation loss 0.0044495356188996756 | validation acc 0.6800262928009033\n",
      "epoch 2 | train loss 0.004463747952363908 | train acc 0.6779306530952454 | validation loss 0.004447118593593679 | validation acc 0.6836734414100647\n",
      "epoch 3 | train loss 0.004458789922380071 | train acc 0.6799623370170593 | validation loss 0.004443505789005977 | validation acc 0.6834143996238708\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004582824037046448 | train acc 0.66144198179245 | validation loss 0.004461520132893811 | validation acc 0.6788902878761292\n",
      "epoch 1 | train loss 0.004471411472818248 | train acc 0.675820529460907 | validation loss 0.0044550802915033945 | validation acc 0.6831353902816772\n",
      "epoch 2 | train loss 0.004463508199103616 | train acc 0.6763603091239929 | validation loss 0.0044491798919863166 | validation acc 0.6817801594734192\n",
      "epoch 3 | train loss 0.004463460496024242 | train acc 0.6776853203773499 | validation loss 0.004452569045813051 | validation acc 0.679109513759613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=32, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005069418102748523 | train acc 0.6058515310287476 | validation loss 0.004597217420992746 | validation acc 0.6716158986091614\n",
      "epoch 1 | train loss 0.004528155480014235 | train acc 0.6707561016082764 | validation loss 0.004451946965988954 | validation acc 0.6833147406578064\n",
      "epoch 2 | train loss 0.004470993669635742 | train acc 0.6781269907951355 | validation loss 0.004442791472906627 | validation acc 0.683195173740387\n",
      "epoch 3 | train loss 0.0044613515777716965 | train acc 0.678342878818512 | validation loss 0.004443389930517641 | validation acc 0.6836535334587097\n",
      "epoch 4 | train loss 0.004452196304802198 | train acc 0.6785489916801453 | validation loss 0.00443882708394976 | validation acc 0.6840122938156128\n",
      "epoch 5 | train loss 0.004454163813831322 | train acc 0.6786667704582214 | validation loss 0.004440363247080573 | validation acc 0.6839525103569031\n",
      "epoch 6 | train loss 0.0044524092853172265 | train acc 0.6792851090431213 | validation loss 0.00443882212412249 | validation acc 0.6840919852256775\n",
      "epoch 7 | train loss 0.004450209004583362 | train acc 0.6793341636657715 | validation loss 0.0044375836456607915 | validation acc 0.6841517686843872\n",
      "epoch 8 | train loss 0.004447554669871776 | train acc 0.6786766052246094 | validation loss 0.004438789099293324 | validation acc 0.6834940910339355\n",
      "epoch 9 | train loss 0.00445050218136134 | train acc 0.6788139939308167 | validation loss 0.0044415536293835964 | validation acc 0.6840919852256775\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005304634000698863 | train acc 0.5619994401931763 | validation loss 0.0050715585904462 | validation acc 0.6253387928009033\n",
      "epoch 1 | train loss 0.004846395652661634 | train acc 0.6425290703773499 | validation loss 0.004623212458385269 | validation acc 0.6627471446990967\n",
      "epoch 2 | train loss 0.004568052345924266 | train acc 0.6661824584007263 | validation loss 0.004484202303538783 | validation acc 0.6790696978569031\n",
      "epoch 3 | train loss 0.004485624396135731 | train acc 0.6772436499595642 | validation loss 0.004451954369350071 | validation acc 0.6827766299247742\n",
      "epoch 4 | train loss 0.0044629388908860295 | train acc 0.6784704923629761 | validation loss 0.0044476447451137465 | validation acc 0.6821588277816772\n",
      "epoch 5 | train loss 0.004455520204577277 | train acc 0.6794617772102356 | validation loss 0.004440744633475147 | validation acc 0.6832549571990967\n",
      "epoch 6 | train loss 0.004451952254412527 | train acc 0.6800702810287476 | validation loss 0.004441470330657095 | validation acc 0.6839724183082581\n",
      "epoch 7 | train loss 0.00444736263109839 | train acc 0.6790593862533569 | validation loss 0.004440521658338815 | validation acc 0.6834741830825806\n",
      "epoch 8 | train loss 0.0044470566223099375 | train acc 0.679648220539093 | validation loss 0.00443816316618147 | validation acc 0.683613657951355\n",
      "epoch 9 | train loss 0.004445987014859288 | train acc 0.6799819469451904 | validation loss 0.004438235604427565 | validation acc 0.6835339665412903\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.00005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.00509055516239134 | train acc 0.6039572954177856 | validation loss 0.004623973122334621 | validation acc 0.6701809763908386\n",
      "epoch 1 | train loss 0.00453963147662931 | train acc 0.6696078181266785 | validation loss 0.004458486681034294 | validation acc 0.6815210580825806\n",
      "epoch 2 | train loss 0.004476245543153151 | train acc 0.6769688129425049 | validation loss 0.004444234651674954 | validation acc 0.6826570630073547\n",
      "epoch 3 | train loss 0.004462162147676245 | train acc 0.6780288219451904 | validation loss 0.004446033453711366 | validation acc 0.6831752061843872\n",
      "epoch 4 | train loss 0.004456018370163192 | train acc 0.6785882711410522 | validation loss 0.004439338907475906 | validation acc 0.6836734414100647\n",
      "epoch 5 | train loss 0.004452455091821357 | train acc 0.6787158250808716 | validation loss 0.004451745376822406 | validation acc 0.6818199753761292\n",
      "epoch 6 | train loss 0.0044524114556732265 | train acc 0.6792163848876953 | validation loss 0.004440945487918465 | validation acc 0.6834940910339355\n",
      "epoch 7 | train loss 0.00445332216600836 | train acc 0.6785980463027954 | validation loss 0.00444087302056854 | validation acc 0.6828762888908386\n",
      "epoch 8 | train loss 0.004446850091583989 | train acc 0.6811498999595642 | validation loss 0.004440495406683743 | validation acc 0.6823780536651611\n",
      "epoch 9 | train loss 0.0044504644089769195 | train acc 0.6803450584411621 | validation loss 0.004441977486879166 | validation acc 0.682039201259613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.2).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  }
 ]
}