{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "display_name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dfc50ec6eac23905ec9d0de14b95fb04de7a27191c56780f2104afc692b71c20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model 1: Attention TRAIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import help_functions\n",
    "import data_processor"
   ]
  },
  {
   "source": [
    "## STRUCTURE\n",
    "3. Build the model with embedding and Attention.\n",
    "5. Train on the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Build the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Get the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"saved_data/splitted_X_train_eq.pickle\")\n",
    "X_test = load_pickle_data(\"saved_data/splitted_X_test_eq.pickle\")\n",
    "y_train = load_pickle_data(\"saved_data/splitted_y_train_eq.pickle\")\n",
    "y_test = load_pickle_data(\"saved_data/splitted_y_test_eq.pickle\")\n",
    "voc = load_pickle_data(\"saved_data/all_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_pickle_data(\"attention_data/splitted_with_mask_X_train.pickle\")\n",
    "X_test = load_pickle_data(\"attention_data/splitted_with_mask_X_test.pickle\")\n",
    "y_train = load_pickle_data(\"attention_data/splitted_with_mask_y_train.pickle\")\n",
    "y_test = load_pickle_data(\"attention_data/splitted_with_mask_y_test.pickle\")\n",
    "mask_train = load_pickle_data(\"attention_data/splitted_with_mask_mask_train.pickle\")\n",
    "mask_test = load_pickle_data(\"attention_data/splitted_with_mask_mask_test.pickle\")\n",
    "voc = load_pickle_data(\"attention_data/splitted_with_mask_voc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training samples: 101905\nNumber of test samples: 50193\n\nSequence length per sample: 283\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {len(y_train)}')\n",
    "print(f'Number of test samples: {len(y_test)}')\n",
    "print(\"\")\n",
    "print(f'Sequence length per sample: {max_sequence_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, batch_size, mask=None):\n",
    "    random_indices = torch.randperm(len(x))\n",
    "    for i in range(0, len(x) - batch_size + 1, batch_size):\n",
    "        indices = random_indices[i:i+batch_size]\n",
    "        if not type(mask) == type(None):\n",
    "            yield x[indices].to(device), y[indices].to(device), mask[indices].to(device)\n",
    "        else:\n",
    "            yield x[indices].to(device), y[indices].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModel(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder = nn.Linear(embedding_dim*max_seq_len, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        # (batch_size, n_tokens, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        #print(f\"Reshaped output shape: {output.shape}\")\n",
    "        output = self.decoder(output)\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        #print(h)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithPooling(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithPooling, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pooler = nn.AvgPool1d(max_seq_len, stride=1)\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        # (batch_size, n_tokens, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        output = self.pooler(output.permute(0,2,1))\n",
    "        #print(f\"Pooled output shape: {output.shape}\")\n",
    "        output = self.decoder(output.view(-1,output.shape[1]))\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        #print(h)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithMaskOnWordPosition(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithMaskOnWordPosition, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "        self.decoder_act = nn.Sigmoid()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, word_position_mask, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        print(f\"Input shape: {src.shape}\")\n",
    "        print(f\"Mask shape: {word_position_mask.shape}\")\n",
    "        print(h)\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        #print(f\"Word position mask shape after unsqueeze: {word_position_mask.unsqueeze(-1).expand(src.shape).shape}\")\n",
    "        src = torch.masked_select(src, word_position_mask.unsqueeze(-1).expand(src.shape)).view(-1, 1, self.embedding_dim)\n",
    "        #print(f\"Masked output shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        output = self.decoder(output)\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        return self.decoder_act(output.squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithMaskOnWordPositionAndSkip(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithMaskOnWordPositionAndSkip, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "        self.batch_normer = torch.nn.BatchNorm1d(embedding_dim)\n",
    "        self.decoder_act = nn.Sigmoid()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, word_position_mask, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        #print(f\"Mask shape: {word_position_mask.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded src shape: {src.shape}\")\n",
    "        skip_src = torch.masked_select(src, word_position_mask.unsqueeze(-1).expand(src.shape)).view(-1, 1, self.embedding_dim)\n",
    "        #print(f\"Skip src shape: {skip_src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        #print(f\"Word position mask shape after unsqueeze: {word_position_mask.unsqueeze(-1).expand(src.shape).shape}\")\n",
    "        src = torch.masked_select(src, word_position_mask.unsqueeze(-1).expand(src.shape)).view(-1, 1, self.embedding_dim)\n",
    "        #print(f\"Masked output shape: {src.shape}\")\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        output = torch.add(output, skip_src).squeeze(1)\n",
    "        #print(f\"Added output shape: {output.shape}\")\n",
    "        output = self.batch_normer(output)\n",
    "        output = self.decoder(output)\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        return self.decoder_act(output)"
   ]
  },
  {
   "source": [
    "## Train the transformer!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by in batchify(X_train, y_train, batch_size):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(sample_weight)\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            accuracy = (output.eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by in batchify(X_test, y_test, batch_size):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mask(model, X_train, X_test, y_train, y_test, mask_train, mask_test, n_epochs=1, batch_size=100, lr=0.001, max_samples=None, weight_true=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    min_ppl = float('inf')\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_fun = F.binary_cross_entropy\n",
    "\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        nbr_train_batches = 0\n",
    "        for bx, by, bm in batchify(X_train, y_train, batch_size, mask_train):\n",
    "            nbr_train_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(bx, bm)\n",
    "            sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "            #print(\"Output after word position mask selection: \")\n",
    "            #print(torch.masked_select(bx, bm))\n",
    "            #print(sample_weight)\n",
    "            #print(f\"by shape: {by.shape}\")\n",
    "            #print(f\"output shape: {output.shape}\")\n",
    "            loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "            loss_sum += loss.item()\n",
    "            #print(f\"output rounded: {output.round()}\")\n",
    "            #print(f\"by: {by}\")\n",
    "            accuracy = (output.round().eq(by)).sum()\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "            if max_samples and updater.n >= max_samples:\n",
    "                break\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss_sum/(nbr_train_batches*batch_size)\n",
    "        train_acc = torch.true_divide(accuracy_sum,(nbr_train_batches*batch_size))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_batches = 0\n",
    "            for bx, by, bm in batchify(X_test, y_test, batch_size, mask_test):\n",
    "                nbr_test_batches += 1\n",
    "                output = model.forward(bx, bm)\n",
    "                sample_weight = (by.eq(1)*weight_true)+(by.eq(0)*(1-weight_true))\n",
    "                loss = loss_fun(output, by.type(torch.FloatTensor), weight=sample_weight)\n",
    "                loss_sum += loss.item()\n",
    "                accuracy = (output.round().eq(by)).sum()\n",
    "                accuracy_sum += accuracy\n",
    "        test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        test_acc = torch.true_divide(accuracy_sum,(nbr_test_batches*batch_size))\n",
    "\n",
    "        print(f'epoch {t} | train loss {train_loss} | train acc {train_acc} | validation loss {test_loss} | validation acc {test_acc}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_subset(sub_percentage, X_train, X_test, y_train, y_test):\n",
    "    train_sub_size = int(sub_percentage*len(y_train))\n",
    "    test_sub_size = int(sub_percentage*len(y_test))\n",
    "\n",
    "    X_train_sub = X_train[:train_sub_size]\n",
    "    X_test_sub = X_test[:test_sub_size]\n",
    "    y_train_sub = y_train[:train_sub_size]\n",
    "    y_test_sub = y_test[:test_sub_size]\n",
    "\n",
    "    return X_train_sub, X_test_sub, y_train_sub, y_test_sub"
   ]
  },
  {
   "source": [
    "### Initial model (BAD)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.7985761088709677 | train acc 0.48891130089759827 | validation loss 0.771484375 | validation acc 0.5062500238418579\n",
      "epoch 1 | train loss 0.8001512096774194 | train acc 0.4879032373428345 | validation loss 0.7584635416666666 | validation acc 0.5145833492279053\n",
      "epoch 2 | train loss 0.8033014112903226 | train acc 0.4858871102333069 | validation loss 0.751953125 | validation acc 0.518750011920929\n",
      "epoch 3 | train loss 0.8048765120967742 | train acc 0.4848790466785431 | validation loss 0.7649739583333334 | validation acc 0.5104166865348816\n",
      "epoch 4 | train loss 0.8001512096774194 | train acc 0.4879032373428345 | validation loss 0.771484375 | validation acc 0.5062500238418579\n"
     ]
    }
   ],
   "source": [
    "data_subset = get_data_subset(0.01, X_train, X_test, y_train, y_test)\n",
    "\n",
    "model = MyAttentionModel(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=32, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train(model, torch.LongTensor(data_subset[0]), torch.LongTensor(data_subset[1]), torch.LongTensor(data_subset[2]), torch.LongTensor(data_subset[3]), n_epochs=5, batch_size=32, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr = 0.0001"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005195849543804742 | train acc 0.5914042592048645 | validation loss 0.004801498082162318 | validation acc 0.6525430679321289\n",
      "epoch 1 | train loss 0.004706355374559161 | train acc 0.6589882969856262 | validation loss 0.004541682469663306 | validation acc 0.6789700388908386\n",
      "epoch 2 | train loss 0.0045658173427119125 | train acc 0.6713548302650452 | validation loss 0.004487742078003512 | validation acc 0.6823381781578064\n",
      "epoch 3 | train loss 0.004529521219671035 | train acc 0.6731312870979309 | validation loss 0.004465106048808927 | validation acc 0.6829759478569031\n",
      "epoch 4 | train loss 0.004496643676869478 | train acc 0.6748292446136475 | validation loss 0.004453594098758541 | validation acc 0.6838328838348389\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr = 0.001"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004614046647859241 | train acc 0.66082364320755 | validation loss 0.004465662476925028 | validation acc 0.6778938174247742\n",
      "epoch 1 | train loss 0.004470847523507976 | train acc 0.6789513826370239 | validation loss 0.0044546720672194486 | validation acc 0.6808235049247742\n",
      "epoch 2 | train loss 0.004456827052998773 | train acc 0.6795108318328857 | validation loss 0.004446458911323654 | validation acc 0.6837332844734192\n",
      "epoch 3 | train loss 0.004451406081546398 | train acc 0.6808750629425049 | validation loss 0.00444886220021563 | validation acc 0.6838528513908386\n",
      "epoch 4 | train loss 0.004447987597978594 | train acc 0.6819056272506714 | validation loss 0.004447791070560925 | validation acc 0.6822983026504517\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr = 0.001, more epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004616558228428686 | train acc 0.6617069840431213 | validation loss 0.0044684916478106565 | validation acc 0.6789700388908386\n",
      "epoch 1 | train loss 0.004466166189782808 | train acc 0.6786962151527405 | validation loss 0.004452353190423978 | validation acc 0.6822983026504517\n",
      "epoch 2 | train loss 0.0044582220102688435 | train acc 0.6807082295417786 | validation loss 0.0044462922137891115 | validation acc 0.6825374960899353\n",
      "epoch 3 | train loss 0.004452732713698835 | train acc 0.6806198954582214 | validation loss 0.0044470997726216875 | validation acc 0.6838528513908386\n",
      "epoch 4 | train loss 0.00444922012106712 | train acc 0.6811989545822144 | validation loss 0.004444805624875791 | validation acc 0.6839525103569031\n",
      "epoch 5 | train loss 0.004448093323419619 | train acc 0.680933952331543 | validation loss 0.00444489560471325 | validation acc 0.6834343075752258\n",
      "epoch 6 | train loss 0.0044408707047903905 | train acc 0.6825631856918335 | validation loss 0.004446119562145418 | validation acc 0.6838727593421936\n",
      "epoch 7 | train loss 0.004442568548549032 | train acc 0.6818761825561523 | validation loss 0.004438682771418529 | validation acc 0.6841916441917419\n",
      "epoch 8 | train loss 0.004439045994071028 | train acc 0.6824159622192383 | validation loss 0.00444401424213037 | validation acc 0.6837531924247742\n",
      "epoch 9 | train loss 0.004438089039943055 | train acc 0.6826319098472595 | validation loss 0.004442521252453194 | validation acc 0.6834940910339355\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr=0.001, embedding_dim=32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004566876791463229 | train acc 0.6648672819137573 | validation loss 0.0044660135632623174 | validation acc 0.6817402839660645\n",
      "epoch 1 | train loss 0.004461310231466146 | train acc 0.6788139939308167 | validation loss 0.004486466950097844 | validation acc 0.6727718710899353\n",
      "epoch 2 | train loss 0.00445315194545529 | train acc 0.6811106204986572 | validation loss 0.0044526342048347755 | validation acc 0.6838528513908386\n",
      "epoch 3 | train loss 0.0044480670664995324 | train acc 0.6816307902336121 | validation loss 0.004455088610150281 | validation acc 0.6832349896430969\n",
      "epoch 4 | train loss 0.004442146246850341 | train acc 0.6825534105300903 | validation loss 0.004455286609151458 | validation acc 0.6823580861091614\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr=0.001, num_heads=2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004608239061415579 | train acc 0.6631497144699097 | validation loss 0.004470323396868034 | validation acc 0.6770169138908386\n",
      "epoch 1 | train loss 0.004469045385514765 | train acc 0.6779306530952454 | validation loss 0.004455907273579066 | validation acc 0.6804049611091614\n",
      "epoch 2 | train loss 0.004453843937872034 | train acc 0.6807769536972046 | validation loss 0.004447608298505656 | validation acc 0.6834542155265808\n",
      "epoch 3 | train loss 0.004449923895416435 | train acc 0.6818369030952454 | validation loss 0.004446920593404591 | validation acc 0.6836934089660645\n",
      "epoch 4 | train loss 0.004448208277846158 | train acc 0.6814345121383667 | validation loss 0.004450966856485157 | validation acc 0.683613657951355\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=2, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with word position (BETTER), lr=0.001, dim_feed_forward=32"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004628330949417798 | train acc 0.6602641940116882 | validation loss 0.00445457686502629 | validation acc 0.6825175285339355\n",
      "epoch 1 | train loss 0.0044671814928495856 | train acc 0.6794813871383667 | validation loss 0.004449236860358612 | validation acc 0.6823979616165161\n",
      "epoch 2 | train loss 0.004461987267585293 | train acc 0.6793930530548096 | validation loss 0.0044524290632190445 | validation acc 0.680086076259613\n",
      "epoch 3 | train loss 0.0044525637734218046 | train acc 0.6799426674842834 | validation loss 0.0044472632630923005 | validation acc 0.6834940910339355\n",
      "epoch 4 | train loss 0.004447900771746066 | train acc 0.6815621256828308 | validation loss 0.004448444883954445 | validation acc 0.6834542155265808\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=32, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model (BETTER), lr=0.001, num_layers=2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.0046285387800173395 | train acc 0.6600973606109619 | validation loss 0.004467735785102396 | validation acc 0.681640625\n",
      "epoch 1 | train loss 0.004472482840977516 | train acc 0.6792752742767334 | validation loss 0.0044523523357216915 | validation acc 0.6832947731018066\n",
      "epoch 2 | train loss 0.00446194501569724 | train acc 0.6793636083602905 | validation loss 0.004453408547287464 | validation acc 0.681620717048645\n",
      "epoch 3 | train loss 0.004452104662251355 | train acc 0.6805708408355713 | validation loss 0.004447732718568771 | validation acc 0.6831552982330322\n",
      "epoch 4 | train loss 0.004451527385287988 | train acc 0.6808750629425049 | validation loss 0.004447868717204761 | validation acc 0.6837332844734192\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPosition(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "### Model with SKIP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004567755323930657 | train acc 0.6651322841644287 | validation loss 0.00446053385991147 | validation acc 0.6810028553009033\n",
      "epoch 1 | train loss 0.004472423260464057 | train acc 0.6766842007637024 | validation loss 0.004462138659080813 | validation acc 0.6785315871238708\n",
      "epoch 2 | train loss 0.004464741773586299 | train acc 0.678382158279419 | validation loss 0.0044527496142925845 | validation acc 0.6825773119926453\n",
      "epoch 3 | train loss 0.004468777842045148 | train acc 0.6767725348472595 | validation loss 0.004450293900195642 | validation acc 0.6780133843421936\n",
      "epoch 4 | train loss 0.004464458588782192 | train acc 0.6788728833198547 | validation loss 0.0044479133179340016 | validation acc 0.6821588277816772\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=1, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004565130642073547 | train acc 0.664739727973938 | validation loss 0.004464899359047603 | validation acc 0.6797273755073547\n",
      "epoch 1 | train loss 0.004474202448990074 | train acc 0.6763995885848999 | validation loss 0.004452605968773631 | validation acc 0.6837531924247742\n",
      "epoch 2 | train loss 0.0044677315027413225 | train acc 0.6775871515274048 | validation loss 0.0044462886476789466 | validation acc 0.6828762888908386\n",
      "epoch 3 | train loss 0.004462914391895803 | train acc 0.6781760454177856 | validation loss 0.004452702448565551 | validation acc 0.6820989847183228\n",
      "epoch 4 | train loss 0.004463629105482859 | train acc 0.6780386567115784 | validation loss 0.004445910977962313 | validation acc 0.6800462603569031\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=5, batch_size=64, lr=0.001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [
    "# PICKED"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004600985258251391 | train acc 0.6613732576370239 | validation loss 0.004453893274850478 | validation acc 0.6813217401504517\n",
      "epoch 1 | train loss 0.004465438979683706 | train acc 0.6777932643890381 | validation loss 0.0044495356188996756 | validation acc 0.6800262928009033\n",
      "epoch 2 | train loss 0.004463747952363908 | train acc 0.6779306530952454 | validation loss 0.004447118593593679 | validation acc 0.6836734414100647\n",
      "epoch 3 | train loss 0.004458789922380071 | train acc 0.6799623370170593 | validation loss 0.004443505789005977 | validation acc 0.6834143996238708\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.004582824037046448 | train acc 0.66144198179245 | validation loss 0.004461520132893811 | validation acc 0.6788902878761292\n",
      "epoch 1 | train loss 0.004471411472818248 | train acc 0.675820529460907 | validation loss 0.0044550802915033945 | validation acc 0.6831353902816772\n",
      "epoch 2 | train loss 0.004463508199103616 | train acc 0.6763603091239929 | validation loss 0.0044491798919863166 | validation acc 0.6817801594734192\n",
      "epoch 3 | train loss 0.004463460496024242 | train acc 0.6776853203773499 | validation loss 0.004452569045813051 | validation acc 0.679109513759613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=32, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=4, batch_size=64, lr=0.0005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005069418102748523 | train acc 0.6058515310287476 | validation loss 0.004597217420992746 | validation acc 0.6716158986091614\n",
      "epoch 1 | train loss 0.004528155480014235 | train acc 0.6707561016082764 | validation loss 0.004451946965988954 | validation acc 0.6833147406578064\n",
      "epoch 2 | train loss 0.004470993669635742 | train acc 0.6781269907951355 | validation loss 0.004442791472906627 | validation acc 0.683195173740387\n",
      "epoch 3 | train loss 0.0044613515777716965 | train acc 0.678342878818512 | validation loss 0.004443389930517641 | validation acc 0.6836535334587097\n",
      "epoch 4 | train loss 0.004452196304802198 | train acc 0.6785489916801453 | validation loss 0.00443882708394976 | validation acc 0.6840122938156128\n",
      "epoch 5 | train loss 0.004454163813831322 | train acc 0.6786667704582214 | validation loss 0.004440363247080573 | validation acc 0.6839525103569031\n",
      "epoch 6 | train loss 0.0044524092853172265 | train acc 0.6792851090431213 | validation loss 0.00443882212412249 | validation acc 0.6840919852256775\n",
      "epoch 7 | train loss 0.004450209004583362 | train acc 0.6793341636657715 | validation loss 0.0044375836456607915 | validation acc 0.6841517686843872\n",
      "epoch 8 | train loss 0.004447554669871776 | train acc 0.6786766052246094 | validation loss 0.004438789099293324 | validation acc 0.6834940910339355\n",
      "epoch 9 | train loss 0.00445050218136134 | train acc 0.6788139939308167 | validation loss 0.0044415536293835964 | validation acc 0.6840919852256775\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.005304634000698863 | train acc 0.5619994401931763 | validation loss 0.0050715585904462 | validation acc 0.6253387928009033\n",
      "epoch 1 | train loss 0.004846395652661634 | train acc 0.6425290703773499 | validation loss 0.004623212458385269 | validation acc 0.6627471446990967\n",
      "epoch 2 | train loss 0.004568052345924266 | train acc 0.6661824584007263 | validation loss 0.004484202303538783 | validation acc 0.6790696978569031\n",
      "epoch 3 | train loss 0.004485624396135731 | train acc 0.6772436499595642 | validation loss 0.004451954369350071 | validation acc 0.6827766299247742\n",
      "epoch 4 | train loss 0.0044629388908860295 | train acc 0.6784704923629761 | validation loss 0.0044476447451137465 | validation acc 0.6821588277816772\n",
      "epoch 5 | train loss 0.004455520204577277 | train acc 0.6794617772102356 | validation loss 0.004440744633475147 | validation acc 0.6832549571990967\n",
      "epoch 6 | train loss 0.004451952254412527 | train acc 0.6800702810287476 | validation loss 0.004441470330657095 | validation acc 0.6839724183082581\n",
      "epoch 7 | train loss 0.00444736263109839 | train acc 0.6790593862533569 | validation loss 0.004440521658338815 | validation acc 0.6834741830825806\n",
      "epoch 8 | train loss 0.0044470566223099375 | train acc 0.679648220539093 | validation loss 0.00443816316618147 | validation acc 0.683613657951355\n",
      "epoch 9 | train loss 0.004445987014859288 | train acc 0.6799819469451904 | validation loss 0.004438235604427565 | validation acc 0.6835339665412903\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.1).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.00005, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 | train loss 0.00509055516239134 | train acc 0.6039572954177856 | validation loss 0.004623973122334621 | validation acc 0.6701809763908386\n",
      "epoch 1 | train loss 0.00453963147662931 | train acc 0.6696078181266785 | validation loss 0.004458486681034294 | validation acc 0.6815210580825806\n",
      "epoch 2 | train loss 0.004476245543153151 | train acc 0.6769688129425049 | validation loss 0.004444234651674954 | validation acc 0.6826570630073547\n",
      "epoch 3 | train loss 0.004462162147676245 | train acc 0.6780288219451904 | validation loss 0.004446033453711366 | validation acc 0.6831752061843872\n",
      "epoch 4 | train loss 0.004456018370163192 | train acc 0.6785882711410522 | validation loss 0.004439338907475906 | validation acc 0.6836734414100647\n",
      "epoch 5 | train loss 0.004452455091821357 | train acc 0.6787158250808716 | validation loss 0.004451745376822406 | validation acc 0.6818199753761292\n",
      "epoch 6 | train loss 0.0044524114556732265 | train acc 0.6792163848876953 | validation loss 0.004440945487918465 | validation acc 0.6834940910339355\n",
      "epoch 7 | train loss 0.00445332216600836 | train acc 0.6785980463027954 | validation loss 0.00444087302056854 | validation acc 0.6828762888908386\n",
      "epoch 8 | train loss 0.004446850091583989 | train acc 0.6811498999595642 | validation loss 0.004440495406683743 | validation acc 0.6823780536651611\n",
      "epoch 9 | train loss 0.0044504644089769195 | train acc 0.6803450584411621 | validation loss 0.004441977486879166 | validation acc 0.682039201259613\n"
     ]
    }
   ],
   "source": [
    "model = MyAttentionModelWithMaskOnWordPositionAndSkip(vocab_size=len(voc), embedding_dim=16, max_seq_len=max_sequence_length, num_heads=1, dim_feedforward=16, num_layers=2, dropout=0.2).to(device)\n",
    "trained_model = train_with_mask(model, torch.LongTensor(X_train), torch.LongTensor(X_test), torch.LongTensor(y_train), torch.LongTensor(y_test), torch.BoolTensor(mask_train), torch.BoolTensor(mask_test), n_epochs=10, batch_size=64, lr=0.0001, max_samples=None, weight_true=0.5)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = input(\"Specify the path you wish to save the Attention model to: \")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  }
 ]
}