{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Evaluate the BERT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Get the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 2. Get the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_data(filename):\n",
    "    with open(filename, \"rb\") as load_file:\n",
    "        return pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_pickle_data(\"BERT_data_final/_evaluation_data.pickle\")"
   ]
  },
  {
   "source": [
    "## 3. Define an evaluator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_BERT(model, eval_data):\n",
    "    print(f\"Model evaluation started!\")\n",
    "    test_acc = {}\n",
    "    model.eval()\n",
    "\n",
    "    for lemma in eval_data.keys():\n",
    "        print(f\"Evaluating model for lemma {lemma}...\")\n",
    "        with torch.no_grad():\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_samples = 0\n",
    "            for dict_entry in eval_data[lemma]:\n",
    "                nbr_test_samples += 1\n",
    "                eval_encodings = tokenizer(dict_entry[\"X_data_1\"], dict_entry[\"X_data_2\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "                outputs = model(**eval_encodings)\n",
    "                #print(f\"output: {output.numpy().flatten()}\")\n",
    "                print(f\"output: {outputs.logits}\")\n",
    "                correct_index_guess = np.argmax(outputs.logits.numpy().flatten())\n",
    "                #print(f\"correct index guess: {correct_index_guess}\")\n",
    "                #loss = loss_fun(output, by.type(torch.FloatTensor))\n",
    "                #loss_sum += loss.item()\n",
    "                accuracy = [1 if correct_index_guess==dict_entry[\"y_data\"][0] else 0]\n",
    "                #print(f\"accuracy: {accuracy}\")\n",
    "                accuracy_sum += accuracy[0]\n",
    "        #test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        #print(h)\n",
    "        test_acc[lemma] = accuracy_sum/nbr_test_samples\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_per_lemma(model, eval_data):\n",
    "    print(f\"Model evaluation started!\")\n",
    "    test_acc = {}\n",
    "    model.eval()\n",
    "    for lemma in eval_data.keys():\n",
    "        print(f\"Evaluating model for lemma {lemma}...\")\n",
    "        with torch.no_grad():\n",
    "            accuracy_sum = 0\n",
    "            nbr_test_samples = 0\n",
    "            for dict_entry in eval_data[lemma]:\n",
    "                nbr_test_samples += 1\n",
    "                output = model.forward(torch.LongTensor(dict_entry[\"X_data\"]).to(device), torch.BoolTensor(dict_entry[\"mask_data\"]).to(device))\n",
    "                #print(f\"output: {output.numpy().flatten()}\")\n",
    "                correct_index_guess = np.argmax(output.numpy().flatten())\n",
    "                #print(f\"correct index guess: {correct_index_guess}\")\n",
    "                #loss = loss_fun(output, by.type(torch.FloatTensor))\n",
    "                #loss_sum += loss.item()\n",
    "                accuracy = [1 if correct_index_guess==dict_entry[\"y_data\"][0] else 0]\n",
    "                #print(f\"accuracy: {accuracy}\")\n",
    "                accuracy_sum += accuracy[0]\n",
    "        #test_loss = loss_sum/(nbr_test_batches*batch_size)\n",
    "        #print(h)\n",
    "        test_acc[lemma] = accuracy_sum/nbr_test_samples\n",
    "    return test_acc"
   ]
  },
  {
   "source": [
    "## 4. Evaluate!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}