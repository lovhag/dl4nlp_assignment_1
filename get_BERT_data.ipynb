{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "display_name": "Python 3.8.5 64-bit ('dl4nlp_assignment_1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dfc50ec6eac23905ec9d0de14b95fb04de7a27191c56780f2104afc692b71c20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# GET BERT DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## CONTENT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. Create \"sense sentences\" for each available sense. The sentence should correctly make use of the sense.\n",
    "2. Create training samples, where each data sample generates two training samples. 1) sample where a sample is paired with a correct sense sentence, 2) where a sample is paired with an incorrect sense sentence.\n",
    "3. Save the data.\n",
    "\n",
    "### In another file\n",
    "4. Obtain a pre-trained BERT.\n",
    "5. Fine-tune BERT on the new data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Create sense sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            sense_key       lemma  word_position  \\\n",
       "0      keep%2:42:07::      keep.v             15   \n",
       "1  national%3:01:00::  national.a             25   \n",
       "2     build%2:31:03::     build.v             38   \n",
       "3     place%1:04:00::     place.n             36   \n",
       "4  position%1:04:01::  position.n             76   \n",
       "\n",
       "                                                text  \n",
       "0  Action by the Committee In pursuance of its ma...  \n",
       "1  A guard of honour stood in formation in honour...  \n",
       "2  The principle that statistics should be timely...  \n",
       "3  Again , he appealed for additional support for...  \n",
       "4  Also , the IAEA has the lowest number of women...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sense_key</th>\n      <th>lemma</th>\n      <th>word_position</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>keep%2:42:07::</td>\n      <td>keep.v</td>\n      <td>15</td>\n      <td>Action by the Committee In pursuance of its ma...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>national%3:01:00::</td>\n      <td>national.a</td>\n      <td>25</td>\n      <td>A guard of honour stood in formation in honour...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>build%2:31:03::</td>\n      <td>build.v</td>\n      <td>38</td>\n      <td>The principle that statistics should be timely...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>place%1:04:00::</td>\n      <td>place.n</td>\n      <td>36</td>\n      <td>Again , he appealed for additional support for...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>position%1:04:01::</td>\n      <td>position.n</td>\n      <td>76</td>\n      <td>Also , the IAEA has the lowest number of women...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "filename = \"/Users/lovhag/Projects/dl4nlp_assignment_1/a1_data/wsd_train.txt\"\n",
    "data = pd.read_table(filename,header=None,names=['sense_key', 'lemma', 'word_position', 'text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 76049 entries, 0 to 76048\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   sense_key      76049 non-null  object\n 1   lemma          76049 non-null  object\n 2   word_position  76049 non-null  int64 \n 3   text           76049 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = data.lemma.unique()\n",
    "sense_dict = {lemma: list(data[data.lemma==lemma].sense_key.unique()) for lemma in lemma_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of available senses: 222\n"
     ]
    }
   ],
   "source": [
    "total_nbr_of_senses = sum([len(sense_dict[key]) for key in sense_dict])\n",
    "print(f\"Total number of available senses: {total_nbr_of_senses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_per_sense = {sense: data[data.sense_key == sense].text.iloc[0] for sense in list(data.sense_key.unique())}\n",
    "sentence_per_lemma_sense = {lemma: {sense: data[data.sense_key == sense].text.iloc[0] for sense in list(data[data.lemma==lemma].sense_key.unique())} for lemma in list(data.lemma.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Action by the Committee In pursuance of its mandate , the Committee will continue to keep under review the situation relating to the question of Palestine and participate in relevant meetings of the General Assembly and the Security Council . The Committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the Occupied Palestinian Territory , including East Jerusalem , requiring international action .'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sentence_per_sense[\"keep%2:42:07::\"]"
   ]
  },
  {
   "source": [
    "## 2. Create training samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pair_data(data):\n",
    "    X_data_1 = [] # pairs!\n",
    "    X_data_2 = []\n",
    "    y_data = []\n",
    "    def add_data_entry(row, sense_key, label):\n",
    "        two_sentences = []\n",
    "        X_data_1.append(row.text)\n",
    "        X_data_2.append(sentence_per_lemma_sense[row.lemma][sense_key])\n",
    "        y_data.append(label)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        add_data_entry(row, row.sense_key, 1)\n",
    "\n",
    "        faulty_senses = list(sentence_per_lemma_sense[row.lemma].keys())\n",
    "        faulty_senses.remove(row.sense_key)\n",
    "        faulty_sense_key = np.random.choice(faulty_senses)\n",
    "        add_data_entry(row, faulty_sense_key, 0)\n",
    "    return X_data_1, X_data_2, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1, X_data_2, y_data = create_sentence_pair_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Action by the Committee In pursuance of its mandate , the Committee will continue to keep under review the situation relating to the question of Palestine and participate in relevant meetings of the General Assembly and the Security Council . The Committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the Occupied Palestinian Territory , including East Jerusalem , requiring international action .'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X_data_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of data samples: 152098\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data samples: {len(X_data_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_with_pickle(data_dict):\n",
    "    pre_filename = input(f\"Specify which prefix filename you wish to save {list(data_dict.keys())} to: \")\n",
    "    if pre_filename:\n",
    "        for key, value in data_dict.items():\n",
    "            filename = pre_filename+\"_\"+key+\".pickle\"\n",
    "            with open(filename, \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(value, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_with_pickle({\"X_data_1\": X_data_1, \"X_data_2\": X_data_2, \"y_data\": y_data})"
   ]
  }
 ]
}