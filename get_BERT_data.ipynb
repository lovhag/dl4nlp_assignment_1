{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET BERT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b0e79804caf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create \"sense sentences\" for each available sense. The sentence should correctly make use of the sense.\n",
    "2. Create training samples, where each data sample generates two training samples. 1) sample where a sample is paired with a correct sense sentence, 2) where a sample is paired with an incorrect sense sentence.\n",
    "3. Save the data.\n",
    "\n",
    "### In another file\n",
    "4. Obtain a pre-trained BERT.\n",
    "5. Fine-tune BERT on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create sense sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/lovhag/Projects/dl4nlp_assignment_1/a1_data/wsd_train.txt\"\n",
    "data = pd.read_table(filename,header=None,names=['sense_key', 'lemma', 'word_position', 'text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = data.lemma.unique()\n",
    "sense_dict = {lemma: list(data[data.lemma==lemma].sense_key.unique()) for lemma in lemma_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nbr_of_senses = sum([len(sense_dict[key]) for key in sense_dict])\n",
    "print(f\"Total number of available senses: {total_nbr_of_senses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_per_sense = {sense: data[data.sense_key == sense].text.iloc[0] for sense in list(data.sense_key.unique())}\n",
    "sentence_per_lemma_sense = {lemma: {sense: data[data.sense_key == sense].text.iloc[0] for sense in list(data[data.lemma==lemma].sense_key.unique())} for lemma in list(data.lemma.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_per_sense[\"keep%2:42:07::\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(range(len(data)), test_size=.2, random_state=42)\n",
    "train_data = data.iloc[train_indices].copy()\n",
    "test_data = data.iloc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pair_data(data):\n",
    "    X_data_1 = [] # pairs!\n",
    "    X_data_2 = []\n",
    "    y_data = []\n",
    "    def add_data_entry(row, sense_key, label):\n",
    "        two_sentences = []\n",
    "        X_data_1.append(row.text)\n",
    "        X_data_2.append(sentence_per_lemma_sense[row.lemma][sense_key])\n",
    "        y_data.append(label)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        add_data_entry(row, row.sense_key, 1)\n",
    "\n",
    "        faulty_senses = list(sentence_per_lemma_sense[row.lemma].keys())\n",
    "        faulty_senses.remove(row.sense_key)\n",
    "        faulty_sense_key = np.random.choice(faulty_senses)\n",
    "        add_data_entry(row, faulty_sense_key, 0)\n",
    "    return X_data_1, X_data_2, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1, X_data_2, y_data = create_sentence_pair_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of data samples for training: {len(X_data_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_with_pickle(data_dict):\n",
    "    pre_filename = input(f\"Specify which prefix filename you wish to save {list(data_dict.keys())} to: \")\n",
    "    if pre_filename:\n",
    "        for key, value in data_dict.items():\n",
    "            filename = pre_filename+\"_\"+key+\".pickle\"\n",
    "            with open(filename, \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(value, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_with_pickle({\"X_data_1_train\": X_data_1, \"X_data_2_train\": X_data_2, \"y_data_train\": y_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1, X_data_2, y_data = create_sentence_pair_data(test_data)\n",
    "print(f\"Number of data samples for testing: {len(X_data_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_with_pickle({\"X_data_1_test\": X_data_1, \"X_data_2_test\": X_data_2, \"y_data_test\": y_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create samples for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_sentence_pair_data(data):\n",
    "    evaluation_data = {}\n",
    "    def add_data_entry(row, sense_key):\n",
    "        two_sentences = []\n",
    "        X_data_1.append(row.text)\n",
    "        X_data_2.append(sentence_per_lemma_sense[row.lemma][sense_key])\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        X_data_1 = [] # pairs!\n",
    "        X_data_2 = []\n",
    "        y_data = [0]\n",
    "\n",
    "        add_data_entry(row, row.sense_key)\n",
    "\n",
    "        faulty_senses = list(sentence_per_lemma_sense[row.lemma].keys())\n",
    "        faulty_senses.remove(row.sense_key)\n",
    "        for faulty_sense_key in faulty_senses:\n",
    "            add_data_entry(row, faulty_sense_key)\n",
    "        \n",
    "        if row.lemma in evaluation_data:\n",
    "            evaluation_data[row.lemma][\"X_data_1\"].append(X_data_1)\n",
    "            evaluation_data[row.lemma][\"X_data_2\"].append(X_data_2)\n",
    "            evaluation_data[row.lemma][\"y_data\"].append(y_data)\n",
    "        else:\n",
    "            evaluation_data[row.lemma] = {}\n",
    "            evaluation_data[row.lemma][\"X_data_1\"] = X_data_1\n",
    "            evaluation_data[row.lemma][\"X_data_2\"] = X_data_2\n",
    "            evaluation_data[row.lemma][\"y_data\"] = y_data\n",
    "\n",
    "    return evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = create_evaluation_sentence_pair_data(test_data)\n",
    "print(f\"Lemmas to evaluate for:\")\n",
    "print(evaluation_data.keys())\n",
    "print(\"\")\n",
    "nbr_of_evaluation_samples_per_lemma = {lemma: len(evaluation_data[lemma][\"X_data_1\"]) for lemma in evaluation_data.keys()}\n",
    "print(f\"Evaluation samples per lemma:\")\n",
    "print(nbr_of_evaluation_samples_per_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_with_pickle({\"evaluation_data\": evaluation_data})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
