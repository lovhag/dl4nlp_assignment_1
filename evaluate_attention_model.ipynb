{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# EVALUATE THE ATTENTION MODEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Get the model to be evaluated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttentionModelWithWordPosition(nn.Module):\n",
    "    \"\"\"My Attention model, based on the Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, dim_feedforward, num_layers, dropout=0.5):\n",
    "        super(MyAttentionModelWithWordPosition, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0.1, max_len=max_seq_len)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward, dropout)\n",
    "        # output shape (batch_size, max_seq_len, embedding_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers) \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pooler = nn.AvgPool1d(max_seq_len, stride=1)\n",
    "        self.decoder = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, word_position_mask, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        #print(f\"Embedded shape: {src.shape}\")\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(f\"Positional encoding shape: {src.shape}\")\n",
    "        ## HERE: change src such that we only take the sequence for the token at WORD POSITION\"\n",
    "        src = torch.masked_select(src, word_position_mask)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(f\"Transformer encoder output shape: {output.shape}\")\n",
    "        # (batch_size, n_tokens, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        output = self.pooler(output.permute(0,2,1))\n",
    "        #print(f\"Pooled output shape: {output.shape}\")\n",
    "        output = self.decoder(output.view(-1,output.shape[1]))\n",
    "        #print(f\"Decoder output shape: {output.shape}\")\n",
    "        #print(h)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyAttentionModelWithWordPosition(vocab_size=len(voc), embedding_dim=32, max_seq_len=max_sequence_length, num_heads=4, dim_feedforward=16, num_layers=1, dropout=0.1)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "source": [
    "## 2. Define an evaluation method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluator_for_sense()"
   ]
  },
  {
   "source": [
    "## 3. Evaluate the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}